{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import RDLogger\n",
    "\n",
    "from SmilesPE.pretokenizer import atomwise_tokenizer\n",
    "from SmilesPE.pretokenizer import kmer_tokenizer\n",
    "from SmilesPE.spe2vec import Corpus\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "import torch\n",
    "\n",
    "supp_script_path = '../../supp_scripts'\n",
    "sys.path.append(supp_script_path) # path for support scripts folder\n",
    "import supp_utils as su\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device,torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# To remove rdkit warning\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole section is to read parameters from the parameter file\n",
    "parameter_filename = \"parameters_molpmofit.json\" \n",
    "parameter_file = open(parameter_filename)\n",
    "parameters = json.load(parameter_file)\n",
    "parameter_file.close()\n",
    "\n",
    "# User inputs\n",
    "input_file_train = parameters[\"input_file_train\"] # input file\n",
    "input_file_test = parameters[\"input_file_test\"] # input file\n",
    "\n",
    "trial = parameters[\"trial\"] # setting False saves the output files else not saved\n",
    "\n",
    "if not trial:\n",
    "    run_folder = parameters[\"run_folder\"]\n",
    "\n",
    "gpu_id = int(parameters[\"gpu_id\"])\n",
    "if gpu_id != None:\n",
    "    device = \"cuda:\" + str(gpu_id)\n",
    "else:\n",
    "    gpu_id = 0\n",
    "\n",
    "torch.cuda.set_device(device)\n",
    "# User inputs\n",
    "trial = parameters[\"trial\"] # setting False saves the output files else not saved\n",
    "\n",
    "k_fold_value = int(parameters[\"k_fold_value\"]) # Number of folds\n",
    "\n",
    "label_wise_augmentation = parameters[\"augmentation\"][\"label_wise_augmentation\"]\n",
    "number_of_augmentation = int(parameters[\"augmentation\"][\"number_of_augmentation\"])\n",
    "iteration = int(parameters[\"augmentation\"][\"iteration\"])\n",
    "\n",
    "tokenization = parameters[\"tokens\"][\"tokenization\"] # options are SPE,atomwise,vocab_file\n",
    "if tokenization == \"SPE\":\n",
    "    spe_token_path = parameters[\"tokens\"][\"spe_token_path\"]\n",
    "else:\n",
    "    spe_token_path = \"\"\n",
    "    \n",
    "#####################\n",
    "# Network parameters#\n",
    "#####################\n",
    "load_model = parameters[\"pretrained_model\"][\"load_model\"]\n",
    "#if load_model is True set the path for pretrained_model_path\n",
    "pretrained_model_path = parameters[\"pretrained_model\"][\"pretrained_model_path\"]\n",
    "pretraining_new_wt = parameters[\"pretrained_model\"][\"pretraining_new_wt\"]\n",
    "pretraining_new_vocab = parameters[\"pretrained_model\"][\"pretraining_new_vocab\"]\n",
    "\n",
    "batch_size = int(parameters[\"network_parameters\"][\"batch_size\"])\n",
    "enable_class_weight = parameters[\"network_parameters\"][\"enable_class_weight\"]\n",
    "\n",
    "Number_of_workers = int(parameters[\"Number_of_workers\"])\n",
    "\n",
    "\n",
    "##################\n",
    "### Do not edit###\n",
    "##################\n",
    "os.system(\"mkdir \" + str(run_folder))\n",
    "\n",
    "atomwise_tokenization = False\n",
    "train_SPE = False\n",
    "\n",
    "if tokenization == \"SPE\":\n",
    "    train_SPE = True\n",
    "else:\n",
    "    atomwise_tokenization = True\n",
    "\n",
    "if not trial:\n",
    "    network_parameter_output = open(str(run_folder) + \"/network_parameters.txt\",\"w\",1)\n",
    "    for parameter in parameters:\n",
    "        network_parameter_output.write(str(parameter) + \" = \" + str(parameters[parameter]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading train_valid and test splits from the file and make it to a dataframe\n",
    "smiles_label_test = {line.split()[0]:line.split()[1] for line in open(input_file_test,\"r\").readlines()}\n",
    "smiles_label_test = dict(sorted(smiles_label_test.items(), key=lambda item: item[1]))\n",
    "\n",
    "smiles_label_train = {line.split()[0]:line.split()[1] for line in open(input_file_train,\"r\").readlines()}\n",
    "smiles_label_train = dict(sorted(smiles_label_train.items(), key=lambda item: item[1]))\n",
    "\n",
    "train_valid_df = su.dict_to_label(smiles_label_train)\n",
    "train_valid_df = train_valid_df\n",
    "test_df = su.dict_to_label(smiles_label_test)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data_path = Path(run_folder)\n",
    "path = data_path\n",
    "path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def get_accuracy(yhat,y):\n",
    "    softmax = torch.exp(yhat.float())\n",
    "    prob = softmax.cpu().detach().numpy()\n",
    "    predictions = np.argmax(prob, axis=1)\n",
    "    y_truth = y.cpu().detach().numpy()\n",
    "    accuracy_check = (y_truth==predictions)\n",
    "    count = np.count_nonzero(accuracy_check)\n",
    "    accuracy = (count/len(accuracy_check))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for fold in range(k_fold_value):\n",
    "    print (\"FOLD \" + str(fold) + \"/\" + str(k_fold_value))\n",
    "    if not trial:\n",
    "        log_file = open(str(run_folder) + \"/model_\" + str(fold) + \".txt\",\"w\")\n",
    "        \n",
    "    piece_count = fold + 1\n",
    "    # create train and valid dataframe of the current fold\n",
    "    train,valid,piece_count = su.CV.get_K_fold_cv_data(train_valid_df,k_fold_value,piece_count,shuffle_output=True)\n",
    "    train_df = pd.DataFrame(train.items(),columns=[\"Smiles\", \"Label\"]).sample(frac=1).reset_index(drop=True)\n",
    "    valid_df = pd.DataFrame(valid.items(),columns=[\"Smiles\", \"Label\"]).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # calculate class_weight\n",
    "    if enable_class_weight:\n",
    "        class_weight = torch.FloatTensor(su.get_class_weight(train_df)).cuda()\n",
    "        if not trial:\n",
    "            log_file.write(\"Class weight for loss (balancing weights)= \" + str(class_weight) + \"\\n\")\n",
    "    \n",
    "    if not trial:\n",
    "        log_file.write(\"Class distribution before augmentation\\n\")\n",
    "        log_file.write(\"Train data\\n\")\n",
    "        log_file.write(str(train_df.groupby('Label').count()) + \"\\n\")\n",
    "        log_file.write(\"Valid data\\n\")\n",
    "        log_file.write(str(valid_df.groupby('Label').count()) + \"\\n\")\n",
    "        log_file.write(\"Test data\\n\")\n",
    "        log_file.write(str(test_df.groupby('Label').count()) + \"\\n\")\n",
    "        \n",
    "    # Data augmentation\n",
    "    if number_of_augmentation > 0:\n",
    "        if label_wise_augmentation:\n",
    "            # label wise augmentation list calculation\n",
    "            train_augmentation_list = su.get_augmentation_list(train_df,number_of_augmentation)\n",
    "            number_of_augmentation_train = train_augmentation_list\n",
    "\n",
    "        else:   \n",
    "            number_of_augmentation_train = number_of_augmentation\n",
    "        \n",
    "        # Augmentation\n",
    "        train_data = su.smiles_augmentation(train_df,\n",
    "                                            N_rounds=number_of_augmentation_train,\n",
    "                                            iteration=iteration,\n",
    "                                            data_set_type=\"train_data\",\n",
    "                                            Number_of_workers=Number_of_workers)     \n",
    "        \n",
    "        if not trial:\n",
    "            log_file.write(\"number of augmentation = \" + str(number_of_augmentation) + \"\\n\")\n",
    "            log_file.write(\"Class distribution after augmentation\\n\")\n",
    "            log_file.write(\"Train data\\n\")\n",
    "            valid_data = valid_df\n",
    "            log_file.write(str(train_data.groupby('Label').count()) + \"\\n\")\n",
    "            log_file.write(\"Valid data\\n\")\n",
    "            log_file.write(str(valid_data.groupby('Label').count()) + \"\\n\")\n",
    "            log_file.write(\"Test data\\n\")\n",
    "            log_file.write(str(valid_data.groupby('Label').count()) + \"\\n\")\n",
    "    else:\n",
    "        train_data = train_df\n",
    "    valid_data = valid_df\n",
    "    test_data = test_df\n",
    "        \n",
    "    # Tokenizer initialization    \n",
    "    if tokenization == \"SPE\":\n",
    "        MolTokenizer = su.molpmofit.MolTokenizer_spe_sos_eos\n",
    "    else:\n",
    "        MolTokenizer = su.molpmofit.MolTokenizer_atomwise_sos_eos\n",
    "\n",
    "    tok = Tokenizer(partial(MolTokenizer,token_path=spe_token_path), n_cpus=Number_of_workers, pre_rules=[], post_rules=[])\n",
    "    \n",
    "    # databunch for training data for vocab for initializing model\n",
    "    qsar_vocab = TextLMDataBunch.from_df(path, train_data, valid_data, bs=batch_size, tokenizer=tok,chunksize=50000, text_cols=0,label_cols=1, max_vocab=60000, include_bos=False)\n",
    "    \n",
    "    # Loading and saving pretrained model with required parameter\n",
    "    pretrained_model_path = Path(pretrained_model_path)\n",
    "\n",
    "    pretrained_fnames = [pretraining_new_wt, pretraining_new_vocab]\n",
    "    fnames = [pretrained_model_path/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]\n",
    "\n",
    "    lm_learner = language_model_learner(qsar_vocab, AWD_LSTM, drop_mult=1.0)\n",
    "    lm_learner = lm_learner.load_pretrained(*fnames)\n",
    "    lm_learner.freeze()\n",
    "    lm_learner.save_encoder(f'lm_encoder')\n",
    "    \n",
    "    # make dataclass (data for classification) using train and valid df\n",
    "    data_clas = TextClasDataBunch.from_df(path, train_data, valid_data, bs=batch_size, tokenizer=tok, \n",
    "                                              chunksize=50000, text_cols='Smiles',label_cols='Label', \n",
    "                                              vocab=qsar_vocab.vocab, max_vocab=60000, include_bos=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    cls_learner = text_classifier_learner(data_clas, AWD_LSTM, pretrained=False, drop_mult=0.2)\n",
    "    # Loading model for finetuning\n",
    "    if enable_class_weight:\n",
    "        cls_learner.loss_func = nn.CrossEntropyLoss(weight=class_weight)\n",
    "\n",
    "        \n",
    "    cls_learner.load_encoder(f'lm_encoder',device=device)\n",
    "    \n",
    "    cls_learner.freeze()\n",
    "    cls_learner.fit_one_cycle(4, 3e-3, moms=(0.8,0.7))\n",
    "    cls_learner.freeze_to(-2)\n",
    "    cls_learner.fit_one_cycle(4, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))\n",
    "    cls_learner.freeze_to(-3)\n",
    "    cls_learner.fit_one_cycle(4, slice(5e-4/(2.6**4),5e-4), moms=(0.8,0.7))\n",
    "    cls_learner.unfreeze()\n",
    "    cls_learner.fit_one_cycle(6, slice(5e-5/(2.6**4),5e-5), moms=(0.8,0.7))\n",
    "    \n",
    "    # Saving model\n",
    "    split_type = \"\"\n",
    "    split_id = \"model_\" + str(fold)\n",
    "    cls_learner.save(f'{split_type}_{split_id}_clas')\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculating statistics for train, valid and test data on the final model of the fold\n",
    "    train_data_clas = TextClasDataBunch.from_df(path, train_data, train_data, bs=batch_size, tokenizer=tok, \n",
    "                              chunksize=50000, text_cols='Smiles',label_cols='Label', vocab=qsar_vocab.vocab, max_vocab=60000,\n",
    "                                              include_bos=False)\n",
    "    learner = text_classifier_learner(train_data_clas, AWD_LSTM, pretrained=False, drop_mult=0.2)\n",
    "    learner.load(f'{split_type}_{split_id}_clas', purge=False);\n",
    "    pred,lbl,loss = learner.get_preds(with_loss=True,ordered=True)\n",
    "    \n",
    "    accuracy = str(get_accuracy(pred,lbl))\n",
    "    loss = str(sum(loss)/len(loss))\n",
    "    softmax = torch.exp(pred.float())\n",
    "    prob = softmax.cpu().detach().numpy()\n",
    "    predictions = np.argmax(prob, axis=1)\n",
    "    y_truth = lbl.cpu().detach().numpy()\n",
    "    target_names = [\"class \" + str(entry) for entry in range(len(set(y_truth)))]\n",
    "    report = classification_report(y_truth, predictions, target_names=target_names,digits=7)\n",
    "    log_file.write(\"\\n\\n\\nTrain data : Accu-\" + str(accuracy) + \"\\tLoss-\" + str(loss) + \"\\n\")\n",
    "    log_file.write(\"Train data report \\n-\" + str(report) + \"\\n\\n\\n\\n\\n\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    valid_data_clas = TextClasDataBunch.from_df(path, train_data, valid_data, bs=batch_size, tokenizer=tok, \n",
    "                              chunksize=50000, text_cols='Smiles',label_cols='Label', vocab=qsar_vocab.vocab, max_vocab=60000,\n",
    "                                              include_bos=False)\n",
    "    learner = text_classifier_learner(valid_data_clas, AWD_LSTM, pretrained=False, drop_mult=0.2)\n",
    "    learner.load(f'{split_type}_{split_id}_clas', purge=False);\n",
    "    pred,lbl,loss = learner.get_preds(with_loss=True,ordered=True)\n",
    "    \n",
    "    accuracy = str(get_accuracy(pred,lbl))\n",
    "    loss = str(sum(loss)/len(loss))\n",
    "    softmax = torch.exp(pred.float())\n",
    "    prob = softmax.cpu().detach().numpy()\n",
    "    predictions = np.argmax(prob, axis=1)\n",
    "    y_truth = lbl.cpu().detach().numpy()\n",
    "    target_names = [\"class \" + str(entry) for entry in range(len(set(y_truth)))]\n",
    "    report = classification_report(y_truth, predictions, target_names=target_names,digits=7)\n",
    "    log_file.write(\"\\n\\n\\nValid data : Accu-\" + str(accuracy) + \"\\tLoss-\" + str(loss) + \"\\n\")\n",
    "    log_file.write(\"Valid data report \\n-\" + str(report) + \"\\n\\n\\n\\n\\n\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    test_data_clas = TextClasDataBunch.from_df(path, train_data, test_data, bs=batch_size, tokenizer=tok, \n",
    "                              chunksize=50000, text_cols='Smiles',label_cols='Label', vocab=qsar_vocab.vocab, max_vocab=60000,\n",
    "                                              include_bos=False)\n",
    "    learner = text_classifier_learner(test_data_clas, AWD_LSTM, pretrained=False, drop_mult=0.2)\n",
    "    learner.load(f'{split_type}_{split_id}_clas', purge=False);\n",
    "    pred,lbl,loss = learner.get_preds(with_loss=True,ordered=True)\n",
    "    \n",
    "    accuracy = str(get_accuracy(pred,lbl))\n",
    "    loss = str(sum(loss)/len(loss))\n",
    "    softmax = torch.exp(pred.float())\n",
    "    prob = softmax.cpu().detach().numpy()\n",
    "    predictions = np.argmax(prob, axis=1)\n",
    "    y_truth = lbl.cpu().detach().numpy()\n",
    "    target_names = [\"class \" + str(entry) for entry in range(len(set(y_truth)))]\n",
    "    report = classification_report(y_truth, predictions, target_names=target_names,digits=7)\n",
    "    log_file.write(\"\\n\\n\\nTest data : Accu-\" + str(accuracy) + \"\\tLoss-\" + str(loss) + \"\\n\")\n",
    "    log_file.write(\"Test data report \\n-\" + str(report) + \"\\n\\n\\n\\n\\n\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qmpred",
   "language": "python",
   "name": "qmpred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
