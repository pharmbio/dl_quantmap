{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to run CNN model, required files are to be downloaded from the link \"http://www.dna.bio.keio.ac.jp/smiles/\" as CNN based model is adapted from Hirohara et al. \n",
    "### Copy the downloaded files to a folder named \"CNN_codes\" and keep the \"CNN_codes\" folder in the folder \"supp_scripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"mkdir ../../supp_scripts/CNN_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading supporting files for the CNN model from Hirohara et al.\n",
    "! wget --quiet http://www.dna.bio.keio.ac.jp/smiles/SCFPfunctions.py -P ../../supp_scripts/CNN_codes \n",
    "! wget --quiet http://www.dna.bio.keio.ac.jp/smiles/SCFPmodel.py -P ../../supp_scripts/CNN_codes\n",
    "! wget --quiet http://www.dna.bio.keio.ac.jp/smiles/evaluate-CV.py -P ../../supp_scripts/CNN_codes\n",
    "! wget --quiet http://www.dna.bio.keio.ac.jp/smiles/evaluate-challenge.py -P ../../supp_scripts/CNN_codes\n",
    "! wget --quiet http://www.dna.bio.keio.ac.jp/smiles/feature.py -P ../../supp_scripts/CNN_codes\n",
    "! wget --quiet http://www.dna.bio.keio.ac.jp/smiles/trainer-CV.py -P ../../supp_scripts/CNN_codes\n",
    "! wget --quiet http://www.dna.bio.keio.ac.jp/smiles/trainer-challenge.py -P ../../supp_scripts/CNN_codes\n",
    "! wget --quiet http://www.dna.bio.keio.ac.jp/smiles/README -P ../../supp_scripts/CNN_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python \n",
    "# coding:utf-8\n",
    "\n",
    "import time, argparse, gc, os\n",
    "import copy\n",
    "import sys\n",
    "import tqdm\n",
    "import json\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, SubsetRandomSampler\n",
    "\n",
    "\n",
    "# custom functions\n",
    "supp_script_path = '../../supp_scripts/'\n",
    "sys.path.append(supp_script_path) # path for support scripts folder\n",
    "sys.path.append(supp_script_path + 'CNN_codes/')\n",
    "from feature import *\n",
    "import SCFPfunctions as Mf\n",
    "\n",
    "import supp_utils as su\n",
    "\n",
    "# set gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device,torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n",
    "\n",
    "# To remove rdkit warning\n",
    "from rdkit import RDLogger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole section is to read parameters from the parameter file\n",
    "parameter_filename = \"parameters_CNN.json\" \n",
    "parameter_file = open(parameter_filename)\n",
    "parameters = json.load(parameter_file)\n",
    "parameter_file.close()\n",
    "\n",
    "# User inputs\n",
    "input_file_train = parameters[\"input_file_train\"] # input file\n",
    "input_file_test = parameters[\"input_file_test\"] # input file\n",
    "\n",
    "trial = parameters[\"trial\"] # setting False saves the output files else not saved\n",
    "\n",
    "if not trial:\n",
    "    run_folder = parameters[\"run_folder\"]\n",
    "\n",
    "gpu_id = int(parameters[\"gpu_id\"])\n",
    "if gpu_id != None:\n",
    "    device = \"cuda:\" + str(gpu_id)\n",
    "else:\n",
    "    gpu_id = 0\n",
    "\n",
    "k_fold_value = int(parameters[\"k_fold_value\"]) # Number of folds\n",
    "\n",
    "lensize = int(parameters[\"sequence_length_cutoff\"][\"lensize\"]) # feature vector size\n",
    "atomsize = int(parameters[\"sequence_length_cutoff\"][\"atomsize\"]) # max length of molecule\n",
    "\n",
    "\n",
    "learning_rate = float(parameters[\"network_parameters\"][\"learning_rate\"])\n",
    "epochs = int(parameters[\"network_parameters\"][\"epochs\"])\n",
    "batchsize = int(parameters[\"network_parameters\"][\"batchsize\"])\n",
    "enable_class_weight = parameters[\"network_parameters\"][\"enable_class_weight\"]\n",
    "\n",
    "# k = window size\n",
    "# s = strides\n",
    "# f = number of filters\n",
    "\n",
    "# first convolution\n",
    "k1 = int(parameters[\"conv1\"][\"k1\"])\n",
    "s1 = int(parameters[\"conv1\"][\"s1\"])\n",
    "f1 = int(parameters[\"conv1\"][\"f1\"])\n",
    "\n",
    "# max pooling 1 \n",
    "k2 = int(parameters[\"pool1\"][\"k2\"])\n",
    "s2 = int(parameters[\"pool1\"][\"s2\"])\n",
    "\n",
    "# second convolution\n",
    "k3 = int(parameters[\"conv2\"][\"k3\"])\n",
    "s3 = int(parameters[\"conv2\"][\"s3\"])\n",
    "f3 = int(parameters[\"conv2\"][\"f3\"])\n",
    "\n",
    "# max pooling 2\n",
    "k4 = int(parameters[\"pool2\"][\"k4\"])\n",
    "s4 = int(parameters[\"pool2\"][\"s4\"])\n",
    "\n",
    "# number of hidden layers\n",
    "n_hid = int(parameters[\"fc_layer_parameters\"][\"n_hid\"])\n",
    "\n",
    "os.system(\"mkdir \" + str(run_folder))\n",
    "if not trial:\n",
    "    network_parameter_output = open(str(run_folder) + \"/network_parameters.txt\",\"w\",1)\n",
    "    for parameter in parameters:\n",
    "        network_parameter_output.write(str(parameter) + \" = \" + str(parameters[parameter]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, atomsize, lensize, k1, s1, f1, k2, s2, k3, \n",
    "                 s3, f3, k4, s4, n_hid, n_out):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, f1, (k1,lensize),padding=(k1//2,0),stride=(s1,s1))\n",
    "        self.bn1 = nn.BatchNorm2d(f1)\n",
    "        self.leakyr = nn.LeakyReLU()\n",
    "        self.pool1 = nn.AvgPool2d((k2,1),stride=(s2,s2),padding=(k2//2,0))\n",
    "        self.conv2 = nn.Conv2d(f1,f3,(k3,1),stride=(s3,s3),padding=(k3//2,0))\n",
    "        self.bn2 = nn.BatchNorm2d(f3)\n",
    "        self.pool2 = nn.AvgPool2d((k4,1),stride=(s4,s4),padding=(k4//2,0))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2),\n",
    "        self.fc1 = nn.Linear(f3, n_hid)\n",
    "        self.fc2 = nn.Linear(n_hid, n_out)\n",
    "        self.bn3 = nn.BatchNorm1d(n_hid)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.leakyr(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.leakyr(self.bn2(self.conv2(x))))\n",
    "        x = F.max_pool2d(x,kernel_size=x.size()[2:])\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout(self.leakyr(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading train_valid and test splits from the file and make it to a dataframe\n",
    "smiles_label_test = {line.split()[0]:line.split()[1] for line in open(input_file_test,\"r\").readlines()}\n",
    "smiles_label_test = dict(sorted(smiles_label_test.items(), key=lambda item: item[1]))\n",
    "\n",
    "smiles_label_train = {line.split()[0]:line.split()[1] for line in open(input_file_train,\"r\").readlines()}\n",
    "smiles_label_train = dict(sorted(smiles_label_train.items(), key=lambda item: item[1]))\n",
    "\n",
    "train_valid_df = su.dict_to_label(smiles_label_train)\n",
    "train_valid_df = train_valid_df\n",
    "test_df = su.dict_to_label(smiles_label_test)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "n_out = len(set(test_df[\"Label\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the size of padding with maximum sequence length and a little\n",
    "# Enable this if all molecules from the data has to be included\n",
    "# else cutoff is taken, mentioned in the parameter file\n",
    "max_sequence_length = max([len(smiles) for smiles in list(smiles_label_train)] + [len(smiles) for smiles in list(smiles_label_test)])\n",
    "if max_sequence_length > atomsize:\n",
    "    atomsize = (max_sequence_length+(100-(max_sequence_length%100))) + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(k_fold_value):\n",
    "    \n",
    "    if not trial:\n",
    "        log_file = open(str(run_folder) + \"/model_\" + str(fold) + \".txt\",\"w\")\n",
    "        model_output_name = str(run_folder) + \"/model_\" + str(fold) + \".pth\"\n",
    "    \n",
    "    piece_count = fold + 1\n",
    "    \n",
    "    # create train and valid set for the fold\n",
    "    train,valid,piece_count = su.CV.get_K_fold_cv_data(train_valid_df,k_fold_value,1,shuffle_output=True)\n",
    "    \n",
    "    # Make feature matrix for train, valid, test\n",
    "    su.write_cid_smiles_output(train,\"train_file.txt\")\n",
    "    x_train,y_train = su.CNN.make_grid(\"train_file.txt\",lensize,atomsize)\n",
    "    \n",
    "    su.write_cid_smiles_output(valid,\"valid_file.txt\")\n",
    "    x_valid,y_valid = su.CNN.make_grid(\"valid_file.txt\",lensize,atomsize)\n",
    "    \n",
    "    su.write_cid_smiles_output(test_df,\"test_file.txt\")\n",
    "    x_test,y_test = su.CNN.make_grid(\"test_file.txt\",lensize,atomsize)\n",
    "\n",
    "    \n",
    "    # calculate class_weight\n",
    "    if enable_class_weight:\n",
    "        class_weight = torch.FloatTensor(su.get_class_weight(train)).cuda(gpu_id)\n",
    "        if not trial:\n",
    "            log_file.write(\"Class weight for loss (balancing weights)= \" + str(class_weight) + \"\\n\")\n",
    "    \n",
    "    # Writing output\n",
    "    if not trial:\n",
    "        train_class_distriution = su.DNN.get_cluster_count_from_label([entry[0] for entry in y_train])\n",
    "        valid_class_distriution = su.DNN.get_cluster_count_from_label([entry[0] for entry in y_valid])\n",
    "        test_class_distriution = su.DNN.get_cluster_count_from_label([entry[0] for entry in y_test])\n",
    "        log_file.write(\"Training : Class distribution = \" + str(train_class_distriution) + \"\\n\")\n",
    "        log_file.write(\"Valid : Class distribution = \" + str(valid_class_distriution) + \"\\n\")\n",
    "        log_file.write(\"Test : Class distribution = \" + str(test_class_distriution) + \"\\n\")\n",
    "    \n",
    "    \n",
    "    # Creating dataloader\n",
    "    train_loader = su.CV.get_dataloader(x_train,y_train.squeeze(),batchsize)\n",
    "    valid_loader = su.CV.get_dataloader(x_valid,y_valid.squeeze(),batchsize)\n",
    "    test_loader = su.CV.get_dataloader(x_test,y_test.squeeze(),batchsize)\n",
    "    \n",
    "    \n",
    "    # initializing network\n",
    "    model = Net(atomsize, lensize, k1, s1, f1, k2, s2, k3, s3, f3, k4, s4, n_hid, n_out)\n",
    "    model.cuda(gpu_id)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    if enable_class_weight:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if not trial:\n",
    "        log_file.write(\"Epochs = \" + str(epochs) + \"\\n\")\n",
    "        log_file.write(\"Learning rate = \" + str(learning_rate) + \"\\n\")\n",
    "        log_file.write(\"optimizer = \" + str(\"Adam\") + \"\\n\")\n",
    "        log_file.write(\"criterion = \" + str(\"CrossEntropyLoss\") + \"\\n\")\n",
    "    \n",
    "    \n",
    "    # Training the network\n",
    "    train_loss_list = []\n",
    "    train_accu_list = []\n",
    "\n",
    "    val_loss_list = []\n",
    "    val_accu_list = []\n",
    "    \n",
    "    train_f1_list = []\n",
    "    valid_f1_list = []\n",
    "    if not trial:\n",
    "        log_file.write(\"Epoch\\tLOSStrain\\tLOSSval\\tACCUtrain\\tACCUval\\n\") \n",
    "\n",
    "    loop = tqdm.tqdm(range(epochs), total=epochs,leave=False)\n",
    "    for epoch in loop:\n",
    "\n",
    "        train_loss, train_accu = su.CNN.train(model,criterion,optimizer,train_loader,device)\n",
    "        val_loss,val_accu = su.CNN.validate(model,criterion,valid_loader,device)\n",
    "        \n",
    "        \n",
    "        # For callback\n",
    "        # Callback saves the best model based on the below priority\n",
    "        # validation loss --> validation accuracy--> training loss--> training accuracy\n",
    "        if epoch == 0: \n",
    "            torch.save(model.state_dict(), model_output_name)\n",
    "            saved_model_id = epoch + 1\n",
    "        \n",
    "        if epoch != 0:\n",
    "            current_epoch_values = [train_loss, train_accu,val_loss,val_accu]\n",
    "            previous_epoch_values = [train_loss_list,train_accu_list,val_loss_list,val_accu_list]\n",
    "            if su.callback(current_epoch_values,previous_epoch_values,model,model_output_name):\n",
    "                model_copy = copy.deepcopy(model)\n",
    "                saved_model_id = epoch + 1\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        train_accu_list.append(train_accu)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_accu_list.append(val_accu)\n",
    "        \n",
    "        if not trial:\n",
    "            log_file.write(str(epoch+1) + \"\\t\" + str(train_loss) + \"\\t\" + str(val_loss) + \"\\t\" + str(train_accu) + \"\\t\" + str(val_accu)  + \"\\n\")\n",
    "        loop.set_description(\"LOSS train:\" + str(train_loss) + \" val:\" + str(val_loss) + \" \\tACCU train:\" + str(train_accu) + \" val:\" + str(val_accu))\n",
    "    \n",
    "    torch.save(model_copy.state_dict(), model_output_name)\n",
    "    \n",
    "    log_file.write(\"\\nChosen model = epoch number \" + str(saved_model_id))\n",
    "    \n",
    "    # Re-initializing the model for getting statistics for all the three sets of data for the current fold\n",
    "    model = Net(atomsize, lensize, k1, s1, f1, k2, s2, k3, s3, f3, k4, s4, n_hid, n_out)\n",
    "    model.load_state_dict(torch.load(model_output_name), strict=True)\n",
    "    model.to(device)\n",
    "\n",
    "    if not trial: # classification report, loss, and accuracy for the datasets\n",
    "        loss,accuracy,prediction_list = su.CNN.test(model,criterion,train_loader,device)\n",
    "        image_name = str(run_folder) + \"/train_\" + str(fold) + \".png\"\n",
    "        report = su.confustion_matrix(prediction_list,image_name)\n",
    "        log_file.write(\"\\n\\n\\nTrain data : Accu-\" + str(accuracy) + \"\\tLoss-\" + str(loss) + \"\\n\")\n",
    "        log_file.write(\"Train data report \\n-\" + str(report) + \"\\n\\n\\n\\n\\n\")\n",
    "    \n",
    "        loss,accuracy,prediction_list = su.CNN.test(model,criterion,valid_loader,device)\n",
    "        image_name = str(run_folder) + \"/valid_\" + str(fold) + \".png\"\n",
    "        report = su.confustion_matrix(prediction_list,image_name)\n",
    "        log_file.write(\"\\n\\n\\nValid data : Accu-\" + str(accuracy) + \"\\tLoss-\" + str(loss) + \"\\n\")\n",
    "        log_file.write(\"Valid data report \\n-\" + str(report) + \"\\n\\n\\n\\n\\n\")\n",
    "        \n",
    "        loss,accuracy,prediction_list = su.CNN.test(model,criterion,test_loader,device)\n",
    "        image_name = str(run_folder) + \"/test_\" + str(fold) + \".png\"\n",
    "        report = su.confustion_matrix(prediction_list,image_name)\n",
    "        log_file.write(\"\\n\\n\\nTest data : Accu-\" + str(accuracy) + \"\\tLoss-\" + str(loss) + \"\\n\")\n",
    "        log_file.write(\"Test data report \\n-\" + str(report) + \"\\n\\n\\n\\n\\n\")\n",
    "        \n",
    "    log_file.close()\n",
    "    \n",
    "    if fold == 0 and not trial:\n",
    "        network_parameter_output.write(\"Used atomsize \" + str(atomsize) + \"\\n\")\n",
    "        network_parameter_output.write(\"model = \" + str(model) + \"\\n\")\n",
    "        network_parameter_output.close()\n",
    "        \n",
    "    # best validation loss\n",
    "    index = val_loss_list.index(sorted(val_loss_list)[0]) # index of least loss\n",
    "    print (\"Best model\")\n",
    "    print (\"LOSS train:\",train_loss_list[index],\" val:\",val_loss_list[index], \"\\tACCU train:\",train_accu_list[index],\" val:\",val_accu_list[index])\n",
    "    print (\"Final model\")    \n",
    "    print (\"LOSS train:\",train_loss,\" val:\",val_loss, \"\\tACCU train:\",train_accu,\" val:\",val_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molpmofit",
   "language": "python",
   "name": "molpmofit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
