{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import glob\n",
    "import tqdm\n",
    "import random\n",
    "import sys\n",
    "import gc\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from rdkit.Chem.AtomPairs import Pairs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "\n",
    "# custom functions\n",
    "supp_script_path = '../../supp_scripts/'\n",
    "sys.path.append(supp_script_path) # path for support scripts folder\n",
    "import supp_utils as su\n",
    "\n",
    "# set gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device,torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n",
    "# To remove rdkit warning\n",
    "from rdkit import RDLogger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_filename = \"parameters_DNN.json\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole section is to read parameters from the parameter file\n",
    "parameter_file = open(parameter_filename)\n",
    "parameters = json.load(parameter_file)\n",
    "parameter_file.close()\n",
    "\n",
    "# User inputs\n",
    "input_file_train = parameters[\"input_file_train\"] # input file\n",
    "input_file_test = parameters[\"input_file_test\"] # input file\n",
    "\n",
    "trial = parameters[\"trial\"] # setting False saves the output files else not saved\n",
    "\n",
    "if not trial:\n",
    "    run_folder = parameters[\"run_folder\"]\n",
    "\n",
    "gpu_id = int(parameters[\"gpu_id\"])\n",
    "if gpu_id != None:\n",
    "    device = \"cuda:\" + str(gpu_id)\n",
    "else:\n",
    "    gpu_id = 0\n",
    "\n",
    "fingerprint_type = parameters[\"fingerprint\"][\"fingerprint_type\"]\n",
    "fingerprint_size = int(parameters[\"fingerprint\"][\"fingerprint_size\"])\n",
    "if fingerprint_type == \"morgan\":\n",
    "    fp_radius = int(parameters[\"fingerprint\"][\"radius\"])\n",
    "\n",
    "# Removing data with lower distribution\n",
    "enable_label_cutoff = parameters[\"label_cutoff\"][\"enable_label_cutoff\"]\n",
    "lower_label_count_cutoff = int(parameters[\"label_cutoff\"][\"lower_label_count_cutoff\"])\n",
    "upper_label_count_cutoff = int(parameters[\"label_cutoff\"][\"upper_label_count_cutoff\"])\n",
    "\n",
    "k_fold_value = int(parameters[\"k_fold_value\"]) # Number of folds\n",
    "\n",
    "epochs = int(parameters[\"network_parameters\"][\"epochs\"])\n",
    "learning_rate = float(parameters[\"network_parameters\"][\"learning_rate\"])\n",
    "batch_size = int(parameters[\"network_parameters\"][\"batch_size\"])\n",
    "enable_class_weight = parameters[\"network_parameters\"][\"enable_class_weight\"]\n",
    "\n",
    "if not trial:\n",
    "    os.system(\"mkdir \" + str(run_folder))\n",
    "\n",
    "if not trial:\n",
    "    network_parameter_output = open(str(run_folder) + \"/network_parameters.txt\",\"w\",1)\n",
    "    for parameter in parameters:\n",
    "        network_parameter_output.write(str(parameter) + \" = \" + str(parameters[parameter]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading input file\n",
    "ML_input = input_file_train\n",
    "with open(ML_input) as f:\n",
    "    f_readlines = f.readlines()\n",
    "    \n",
    "\n",
    "# Finding cluster distribution\n",
    "num_classes = set([label.strip().split(\" \")[1] for label in f_readlines])\n",
    "class_distriution = su.DNN.get_cluster_count_from_label([label.strip().split(\" \")[1] for label in f_readlines])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,p1=0.0,p2=0.0,output_classes=2):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1024, 4096)\n",
    "        self.drop1 = nn.Dropout(p=p1)\n",
    "        self.bn1 = nn.BatchNorm1d(4096)\n",
    "        \n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.drop2 = nn.Dropout(p=p2)\n",
    "        self.bn2 = nn.BatchNorm1d(4096)\n",
    "        \n",
    "        self.fc3 = nn.Linear(4096, 1024)\n",
    "        self.drop3 = nn.Dropout(p=p2)\n",
    "\n",
    "        self.fc4 = nn.Linear(1024, output_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.relu(self.bn1(self.fc1(x))))\n",
    "        \n",
    "        x = self.drop2(self.relu(self.bn2(self.fc2(x))))\n",
    "        \n",
    "        x = self.drop3(self.relu((self.fc3(x))))\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading train_valid and test splits from the file and make it to a dataframe\n",
    "smiles_label_test = {line.split()[0]:line.split()[1] for line in open(input_file_test,\"r\").readlines()}\n",
    "smiles_label_test = dict(sorted(smiles_label_test.items(), key=lambda item: item[1]))\n",
    "\n",
    "smiles_label_train = {line.split()[0]:line.split()[1] for line in open(input_file_train,\"r\").readlines()}\n",
    "smiles_label_train = dict(sorted(smiles_label_train.items(), key=lambda item: item[1]))\n",
    "label_count = set(list(smiles_label_train.values()))\n",
    "\n",
    "train_valid_df = su.dict_to_label(smiles_label_train)\n",
    "train_valid_df = train_valid_df\n",
    "test_df = su.dict_to_label(smiles_label_test)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "print (len(train_valid_df),len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(k_fold_value):\n",
    "    \n",
    "    if not trial:\n",
    "        log_file = open(str(run_folder) + \"/model_\" + str(fold) + \".txt\",\"w\")\n",
    "        model_output_name = str(run_folder) + \"/model_\" + str(fold) + \".pth\" \n",
    "        \n",
    "    piece_count = fold + 1\n",
    "    \n",
    "    # create train and valid set for the fold\n",
    "    train,valid,piece_count = su.CV.get_K_fold_cv_data(train_valid_df,k_fold_value,piece_count,shuffle_output=True)\n",
    "    \n",
    "    # Get fingerprint for train, valid, test\n",
    "    x_train_fp,y_train_fp = su.DNN.smiles_to_fp(train,fingerprint_type,fp_radius,fingerprint_size)\n",
    "    x_valid_fp,y_valid_fp = su.DNN.smiles_to_fp(valid,fingerprint_type,fp_radius,fingerprint_size)\n",
    "    x_test_fp,y_test_fp = su.DNN.smiles_to_fp(test_df,fingerprint_type,fp_radius,fingerprint_size)\n",
    "    \n",
    "    \n",
    "    # Creating dataloader\n",
    "    train_loader = su.CV.get_dataloader(x_train_fp,y_train_fp,batch_size)\n",
    "    valid_loader = su.CV.get_dataloader(x_valid_fp,y_valid_fp,batch_size)\n",
    "    test_loader = su.CV.get_dataloader(x_test_fp,y_test_fp,batch_size)\n",
    "    \n",
    "    # Writing output\n",
    "    if not trial:\n",
    "        train_class_distriution = su.DNN.get_cluster_count_from_label(y_train_fp)\n",
    "        valid_class_distriution = su.DNN.get_cluster_count_from_label(y_valid_fp)\n",
    "        test_class_distriution = su.DNN.get_cluster_count_from_label(y_test_fp)\n",
    "        log_file.write(\"Training : Class distribution = \" + str(train_class_distriution) + \"\\n\")\n",
    "        log_file.write(\"Valid : Class distribution = \" + str(valid_class_distriution) + \"\\n\")\n",
    "        log_file.write(\"Test : Class distribution = \" + str(test_class_distriution) + \"\\n\")\n",
    "    \n",
    "    # calculate class_weight\n",
    "    if enable_class_weight:\n",
    "        class_weight = torch.FloatTensor(su.get_class_weight(train)).cuda(gpu_id)\n",
    "        if not trial:\n",
    "            log_file.write(\"Class weight for loss (balancing weights)= \" + str(class_weight) + \"\\n\")\n",
    "    \n",
    "    # initializing network\n",
    "    model = Net(p1=0.4,p2=0.4,output_classes=len(label_count))\n",
    "    model.cuda(gpu_id)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    if enable_class_weight:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    # Training the network\n",
    "    # List to store values\n",
    "    train_loss_list = []\n",
    "    train_accu_list = []\n",
    "\n",
    "    val_loss_list = []\n",
    "    val_accu_list = []\n",
    "    \n",
    "    train_f1_list = []\n",
    "    valid_f1_list = []\n",
    "    # model training\n",
    "    loop = tqdm.tqdm(range(epochs), total=epochs,leave=False)\n",
    "    for epoch in loop:\n",
    "\n",
    "        train_loss, train_accu = su.DNN.train(model,criterion,optimizer,train_loader,device)\n",
    "        val_loss,val_accu = su.DNN.validate(model,criterion,valid_loader,device)\n",
    "        \n",
    "        \n",
    "        # For callback\n",
    "        # Callback saves the best model based on the below priority\n",
    "        # validation loss --> validation accuracy--> training loss--> training accuracy\n",
    "        if epoch == 0:\n",
    "            torch.save(model.state_dict(), model_output_name)\n",
    "            saved_model_id = epoch + 1\n",
    "            \n",
    "        if epoch != 0:\n",
    "            current_epoch_values = [train_loss, train_accu,val_loss,val_accu]\n",
    "            previous_epoch_values = [train_loss_list,train_accu_list,val_loss_list,val_accu_list]\n",
    "            if su.callback(current_epoch_values,previous_epoch_values,model,model_output_name):\n",
    "                model_copy = copy.deepcopy(model)\n",
    "                saved_model_id = epoch + 1\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        train_accu_list.append(train_accu)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_accu_list.append(val_accu)\n",
    "        \n",
    "        if not trial:\n",
    "            log_file.write(str(epoch+1) + \"\\t\" + str(train_loss) + \"\\t\" + str(val_loss) + \"\\t\" + str(train_accu) + \"\\t\" + str(val_accu)  + \"\\n\")\n",
    "        loop.set_description(\"LOSS train:\" + str(train_loss) + \" val:\" + str(val_loss) + \" \\tACCU train:\" + str(train_accu) + \" val:\" + str(val_accu))\n",
    "    \n",
    "    torch.save(model_copy.state_dict(), model_output_name)\n",
    "    \n",
    "    log_file.write(\"\\nChosen model = epoch number \" + str(saved_model_id))\n",
    "    \n",
    "    \n",
    "    # Re-initializing the model for getting statistics for all the three sets of data for the current fold\n",
    "    model = Net(p1=0.4,p2=0.4,output_classes=len(label_count))\n",
    "    model.load_state_dict(torch.load(model_output_name), strict=True)\n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "    if not trial: # classification report and confusion matrix plot\n",
    "        loss,accuracy,prediction_list = su.DNN.test(model,criterion,train_loader,device)\n",
    "        image_name = str(run_folder) + \"/train_\" + str(fold) + \".png\"\n",
    "        report = su.confustion_matrix(prediction_list,image_name)\n",
    "        log_file.write(\"\\n\\n\\nTrain data : Accu-\" + str(accuracy) + \"\\tLoss-\" + str(loss) + \"\\n\")\n",
    "        log_file.write(\"Train data report \\n-\" + str(report) + \"\\n\\n\\n\\n\\n\")\n",
    "    \n",
    "        loss,accuracy,prediction_list = su.DNN.test(model,criterion,valid_loader,device)\n",
    "        image_name = str(run_folder) + \"/valid_\" + str(fold) + \".png\"\n",
    "        report = su.confustion_matrix(prediction_list,image_name)\n",
    "        log_file.write(\"\\n\\n\\nValid data : Accu-\" + str(accuracy) + \"\\tLoss-\" + str(loss) + \"\\n\")\n",
    "        log_file.write(\"Valid data report \\n-\" + str(report) + \"\\n\\n\\n\\n\\n\")\n",
    "        \n",
    "        loss,accuracy,prediction_list = su.DNN.test(model,criterion,test_loader,device)\n",
    "        image_name = str(run_folder) + \"/test_\" + str(fold) + \".png\"\n",
    "        report = su.confustion_matrix(prediction_list,image_name)\n",
    "        log_file.write(\"\\n\\n\\nTest data : Accu-\" + str(accuracy) + \"\\tLoss-\" + str(loss) + \"\\n\")\n",
    "        log_file.write(\"Test data report \\n-\" + str(report) + \"\\n\\n\\n\\n\\n\")\n",
    "    \n",
    "    log_file.close()\n",
    "    \n",
    "    if fold == 0 and not trial:\n",
    "        network_parameter_output.write(\"model = \" + str(model) + \"\\n\")\n",
    "        network_parameter_output.close()\n",
    "    \n",
    "    # best validation loss\n",
    "    index = val_loss_list.index(sorted(val_loss_list)[0]) # index of least loss\n",
    "    print (\"Best model\")\n",
    "    print (\"LOSS train:\",train_loss_list[index],\" val:\",val_loss_list[index], \"\\tACCU train:\",train_accu_list[index],\" val:\",val_accu_list[index])\n",
    "    print (\"Final model\")    \n",
    "    print (\"LOSS train:\",train_loss,\" val:\",val_loss, \"\\tACCU train:\",train_accu,\" val:\",val_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molpmofit",
   "language": "python",
   "name": "molpmofit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
