{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import gc\n",
    "import glob\n",
    "import sys\n",
    "import random\n",
    "import string\n",
    "import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import codecs\n",
    "from SmilesPE.pretokenizer import atomwise_tokenizer\n",
    "from SmilesPE.pretokenizer import kmer_tokenizer\n",
    "from SmilesPE.learner import *\n",
    "from SmilesPE.tokenizer import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "supp_script_path = '//'\n",
    "sys.path.append(supp_script_path) # path for support scripts folder\n",
    "import supp_utils as su\n",
    "\n",
    "# set gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device,torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# To remove rdkit warning\n",
    "from rdkit import RDLogger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_filename = \"parameters_seq2seq.json\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole section is to read parameters from the parameter file\n",
    "parameter_file = open(parameter_filename)\n",
    "parameters = json.load(parameter_file)\n",
    "parameter_file.close()\n",
    "\n",
    "# User inputs\n",
    "input_file_train = parameters[\"input_file_train\"] # input file\n",
    "input_file_test = parameters[\"input_file_test\"] # input file\n",
    "\n",
    "trial = parameters[\"trial\"] # setting False saves the output files else not saved\n",
    "\n",
    "if not trial:\n",
    "    run_folder = parameters[\"run_folder\"]\n",
    "\n",
    "gpu_id = int(parameters[\"gpu_id\"])\n",
    "if gpu_id != None:\n",
    "    device = \"cuda:\" + str(gpu_id)\n",
    "else:\n",
    "    gpu_id = 0\n",
    "    \n",
    "# Removing data with lower distribution\n",
    "enable_label_cutoff = parameters[\"label_cutoff\"][\"enable_label_cutoff\"]\n",
    "lower_label_count_cutoff = int(parameters[\"label_cutoff\"][\"lower_label_count_cutoff\"])\n",
    "upper_label_count_cutoff = int(parameters[\"label_cutoff\"][\"upper_label_count_cutoff\"])\n",
    "\n",
    "# Sequence length to be considered\n",
    "lower_cutoff = int(parameters[\"sequence_length_cutoff\"][\"lower_cutoff\"])\n",
    "upper_cutoff = int(parameters[\"sequence_length_cutoff\"][\"upper_cutoff\"])\n",
    "\n",
    "k_fold_value = int(parameters[\"k_fold_value\"]) # Number of folds\n",
    "\n",
    "test_set_percentage = float(parameters[\"test_set_percentage\"])\n",
    "\n",
    "label_wise_augmentation = parameters[\"augmentation\"][\"label_wise_augmentation\"]\n",
    "number_of_augmentation = int(parameters[\"augmentation\"][\"number_of_augmentation\"])\n",
    "iteration = int(parameters[\"augmentation\"][\"iteration\"])\n",
    "\n",
    "tokenization = parameters[\"tokens\"][\"tokenization\"] # options are SPE,atomwise,vocab_file\n",
    "if tokenization == \"SPE\":\n",
    "    spe_min_frequency = int(parameters[\"tokens\"][\"spe_min_frequency\"])\n",
    "sos_eos_tokens = parameters[\"tokens\"][\"sos_eos_tokens\"]\n",
    "\n",
    "use_vocab_file = parameters[\"vocab_file\"][\"use_vocab_file\"]\n",
    "if use_vocab_file:\n",
    "    vocab_file_path = parameters[\"vocab_file\"][\"vocab_file_path\"]\n",
    "\n",
    "#####################\n",
    "# Network parameters#\n",
    "#####################\n",
    "load_model = parameters[\"pretrained_model\"][\"load_model\"]\n",
    "#if load_model is True set the path for pretrained_model_path\n",
    "pretrained_model_path = parameters[\"pretrained_model\"][\"pretrained_model_path\"]\n",
    "\n",
    "hidden_size = int(parameters[\"lstm_parameters\"][\"hidden_size\"])\n",
    "num_layers = int(parameters[\"lstm_parameters\"][\"num_layers\"])\n",
    "en_embedding_size = int(parameters[\"lstm_parameters\"][\"en_embedding_size\"])\n",
    "en_dropout = float(parameters[\"lstm_parameters\"][\"en_dropout\"])\n",
    "\n",
    "fc_size = int(parameters[\"fc_layer_parameters\"][\"fc_size\"])\n",
    "fc_dropout = float(parameters[\"fc_layer_parameters\"][\"fc_dropout\"]) # fully connected layer dropout\n",
    "\n",
    "epochs = int(parameters[\"network_parameters\"][\"epochs\"])\n",
    "batch_size = int(parameters[\"network_parameters\"][\"batch_size\"])\n",
    "learning_rate = float(parameters[\"network_parameters\"][\"learning_rate\"])\n",
    "enable_class_weight = parameters[\"network_parameters\"][\"enable_class_weight\"]\n",
    "\n",
    "Number_of_workers = int(parameters[\"Number_of_workers\"])\n",
    "\n",
    "##################\n",
    "### Do not edit###\n",
    "##################\n",
    "if not trial:\n",
    "    os.system(\"mkdir \" + str(run_folder))\n",
    "\n",
    "atomwise_tokenization = False\n",
    "train_SPE = False\n",
    "\n",
    "if tokenization == \"SPE\":\n",
    "    train_SPE = True\n",
    "elif tokenization == \"atomwise\":\n",
    "    atomwise_tokenization = True\n",
    "else:\n",
    "    atomwise_tokenization = True\n",
    "    print (\"Tokenization not provided/incorrect. Using atomwise tokenization\")\n",
    "\n",
    "if not trial:\n",
    "    network_parameter_output = open(str(run_folder) + \"/network_parameters.txt\",\"w\",1)\n",
    "    for parameter in parameters:\n",
    "        network_parameter_output.write(str(parameter) + \" = \" + str(parameters[parameter]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq network, only with the perceiver connected to the fully connected layer for classification\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size) #,padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p,batch_first=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (N,seq_length) where N is batch size\n",
    "          \n",
    "        pack_pad_list = x.shape[0] - (np.array(x.cpu()) == 0).sum(0)\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (N,seq_length, embedding_size)\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedding, \n",
    "                                                                lengths = torch.as_tensor(pack_pad_list, dtype=torch.int64).cpu(),\n",
    "                                                                batch_first = False,\n",
    "                                                                enforce_sorted=False\n",
    "                                                               )\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        return hidden[hidden.shape[0]-1]\n",
    "        \n",
    "class FC_layer(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,p):\n",
    "        super(FC_layer, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, fc_size)   \n",
    "        self.fc2 = nn.Linear(fc_size, output_size)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        fc_out = self.dropout(self.lrelu((self.fc1(hidden))))\n",
    "        fc_out = self.fc2(fc_out)\n",
    "        \n",
    "        return fc_out\n",
    "\n",
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, fc_layer):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc_layer = fc_layer\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "\n",
    "        hidden = self.encoder(source)\n",
    "        outputs = self.fc_layer(hidden)        \n",
    "        #outputs = self.softmax(outputs)\n",
    "        \n",
    "        return outputs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading train_valid and test splits from the file and make it to a dataframe\n",
    "smiles_label_test = {line.split()[0]:line.split()[1] for line in open(input_file_test,\"r\").readlines()}\n",
    "smiles_label_test = dict(sorted(smiles_label_test.items(), key=lambda item: item[1]))\n",
    "\n",
    "smiles_label_train = {line.split()[0]:line.split()[1] for line in open(input_file_train,\"r\").readlines()}\n",
    "smiles_label_train = dict(sorted(smiles_label_train.items(), key=lambda item: item[1]))\n",
    "\n",
    "train_valid_df = su.dict_to_label(smiles_label_train)\n",
    "train_valid_df = train_valid_df\n",
    "test_df = su.dict_to_label(smiles_label_test)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def plot_loss_accuracy(train_list,valid_list,filename):\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    plt.plot(train_list, label='Training')\n",
    "    plt.plot(valid_list, label='Validation')\n",
    "    plt.legend()\n",
    "    plt.savefig(filename)\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(k_fold_value):\n",
    "    \n",
    "    if not trial:\n",
    "        log_file = open(str(run_folder) + \"/model_\" + str(fold) + \".txt\",\"w\")\n",
    "        model_output_name = str(run_folder) + \"/model_\" + str(fold) + \".pth\"\n",
    "    \n",
    "    piece_count = fold + 1\n",
    "    # create train and valid dataframe for the current fold\n",
    "    train,valid,piece_count = su.CV.get_K_fold_cv_data(train_valid_df,k_fold_value,piece_count,shuffle_output=True)\n",
    "    train_df = pd.DataFrame(train.items(),columns=[\"Smiles\", \"Label\"]).sample(frac=1).reset_index(drop=True)\n",
    "    valid_df = pd.DataFrame(valid.items(),columns=[\"Smiles\", \"Label\"]).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # calculate class_weight\n",
    "    if enable_class_weight:\n",
    "        class_weight = torch.FloatTensor(su.get_class_weight(train_df)).cuda(gpu_id)\n",
    "        if not trial:\n",
    "            log_file.write(\"Class weight for loss (balancing weights)= \" + str(class_weight) + \"\\n\")\n",
    "            \n",
    "    if not trial:\n",
    "        log_file.write(\"Class distribution before augmentation\\n\")\n",
    "        log_file.write(\"Train data\\n\")\n",
    "        log_file.write(str(train_df.groupby('Label').count()) + \"\\n\")\n",
    "        log_file.write(\"Valid data\\n\")\n",
    "        log_file.write(str(valid_df.groupby('Label').count()) + \"\\n\")\n",
    "        log_file.write(\"Test data\\n\")\n",
    "        log_file.write(str(test_df.groupby('Label').count()) + \"\\n\")\n",
    "    \n",
    "    \n",
    "    # Data augmentation\n",
    "    if number_of_augmentation > 0:\n",
    "        if label_wise_augmentation:\n",
    "            # label wise augmentation list calculation\n",
    "            train_augmentation_list = su.get_augmentation_list(train_df,number_of_augmentation)\n",
    "            number_of_augmentation_train = train_augmentation_list\n",
    "            \n",
    "            valid_augmentation_list = su.get_augmentation_list(valid_df,number_of_augmentation)\n",
    "            number_of_augmentation_valid = valid_augmentation_list\n",
    "            \n",
    "            #if fold == 0:\n",
    "            test_augmentation_list = su.get_augmentation_list(test_df,number_of_augmentation)\n",
    "            number_of_augmentation_test = test_augmentation_list\n",
    "\n",
    "        else:   \n",
    "            number_of_augmentation_train = number_of_augmentation\n",
    "            number_of_augmentation_valid = number_of_augmentation\n",
    "            #if fold == 0:\n",
    "            number_of_augmentation_test = number_of_augmentation\n",
    "                \n",
    "        train_data = su.smiles_augmentation(train_df,\n",
    "                                            N_rounds=number_of_augmentation_train,\n",
    "                                            iteration=iteration,\n",
    "                                            data_set_type=\"train_data\",\n",
    "                                            Number_of_workers=Number_of_workers)     \n",
    "            \n",
    "        valid_data = su.smiles_augmentation(valid_df,\n",
    "                                            N_rounds=number_of_augmentation_valid,\n",
    "                                            iteration=iteration,\n",
    "                                            data_set_type=\"valid_data\",\n",
    "                                            Number_of_workers=Number_of_workers)\n",
    "        #if fold == 0:\n",
    "        test_data = su.smiles_augmentation(test_df,\n",
    "                                            N_rounds=number_of_augmentation_test,\n",
    "                                            iteration=iteration,\n",
    "                                            data_set_type=\"test_data\",\n",
    "                                            Number_of_workers=Number_of_workers)\n",
    "        \n",
    "        if not trial:\n",
    "            log_file.write(\"number of augmentation = \" + str(number_of_augmentation) + \"\\n\")\n",
    "            log_file.write(\"Class distribution after augmentation\\n\")\n",
    "            log_file.write(\"Train data\\n\")\n",
    "            log_file.write(str(train_data.groupby('Label').count()) + \"\\n\")\n",
    "            log_file.write(\"Valid data\\n\")\n",
    "            log_file.write(str(valid_data.groupby('Label').count()) + \"\\n\")\n",
    "            log_file.write(\"Test data\\n\")\n",
    "            log_file.write(str(valid_data.groupby('Label').count()) + \"\\n\")\n",
    "    else:\n",
    "        train_data = train_df\n",
    "        valid_data = valid_df\n",
    "        #if fold == 0:\n",
    "        test_data = test_df\n",
    "    \n",
    "    \n",
    "    # train spe tokenization\n",
    "    if train_SPE:\n",
    "        all_smiles = train_data['Smiles'].to_list()\n",
    "        token_path = str(run_folder) + \"/tokens_\" + str(fold) + \".txt\"\n",
    "        su.seq2seq.train_spe_tokenizer(all_smiles,token_path,min_frequency=spe_min_frequency,augmentation=0)\n",
    "    \n",
    "    \n",
    "    # create or use vocab\n",
    "    if use_vocab_file:\n",
    "        word_index,index_word = su.seq2seq.read_vocab_file(vocab_file_path)\n",
    "        token_path = \"\"\n",
    "    else:\n",
    "        output_vocab_path = str(run_folder) + \"/vocab\" + str(fold) + \".txt\"\n",
    "        if train_SPE:\n",
    "            word_index,index_word = su.seq2seq.create_vocab_file_spe(train_data,\n",
    "                                                                     token_path,\n",
    "                                                                     Number_of_workers,\n",
    "                                                                     output_vocab_path,\n",
    "                                                                     sos_eos_tokens)\n",
    "        else:\n",
    "            token_path = \"\"\n",
    "            word_index,index_word = su.seq2seq.create_vocab_file_atomwise(train_data,\n",
    "                                                                          Number_of_workers,\n",
    "                                                                          output_vocab_path,\n",
    "                                                                          sos_eos_tokens)\n",
    "    \n",
    "    \n",
    "    # convert to tokens\n",
    "    x_train,y_train= su.seq2seq.convert_smiles_to_tokens(train_data,\n",
    "                                                         lower_cutoff=lower_cutoff,\n",
    "                                                         upper_cutoff=upper_cutoff,\n",
    "                                                         Number_of_workers=Number_of_workers,\n",
    "                                                         token_path=token_path,\n",
    "                                                         sos_eos_tokens=sos_eos_tokens,\n",
    "                                                         tokenization=tokenization)\n",
    "    \n",
    "    x_valid,y_valid= su.seq2seq.convert_smiles_to_tokens(valid_data,\n",
    "                                                       lower_cutoff=lower_cutoff,\n",
    "                                                       upper_cutoff=upper_cutoff,\n",
    "                                                       Number_of_workers=Number_of_workers,\n",
    "                                                       token_path=token_path,\n",
    "                                                       sos_eos_tokens=sos_eos_tokens,\n",
    "                                                       tokenization=tokenization)\n",
    "    \n",
    "    #if fold == 0:\n",
    "    x_test,y_test= su.seq2seq.convert_smiles_to_tokens(test_data,\n",
    "                                                       lower_cutoff=lower_cutoff,\n",
    "                                                       upper_cutoff=upper_cutoff,\n",
    "                                                       Number_of_workers=Number_of_workers,\n",
    "                                                       token_path=token_path,\n",
    "                                                       sos_eos_tokens=sos_eos_tokens,\n",
    "                                                       tokenization=tokenization)\n",
    "    \n",
    "    if not trial:\n",
    "        log_file.write(\"Training : Data point = \" + str(len(train_df)) + \"\\n\")\n",
    "        log_file.write(\"Training : Data point within cutoff criteria = \" + str(len(x_train)) + \"\\n\")\n",
    "        log_file.write(\"Valid : Data point = \" + str(len(valid_df)) + \"\\n\")\n",
    "        log_file.write(\"Valid : Data point within cutoff criteria = \" + str(len(x_valid)) + \"\\n\")\n",
    "        log_file.write(\"Test : Data point = \" + str(len(test_df)) + \"\\n\")\n",
    "        log_file.write(\"Test : Data point within cutoff criteria = \" + str(len(x_test)) + \"\\n\")\n",
    "    \n",
    "    \n",
    "    # convert to index\n",
    "    x_train_indexed = su.seq2seq.convert_token_to_index_multi(x_train,word_index)\n",
    "    x_valid_indexed = su.seq2seq.convert_token_to_index_multi(x_valid,word_index)\n",
    "    #if fold == 0:\n",
    "    x_test_indexed = su.seq2seq.convert_token_to_index_multi(x_test,word_index)\n",
    "    \n",
    "    # create iterator\n",
    "    train_iterator =  su.seq2seq.make_bucket_iterator(x_train_indexed,y_train,batch_size,device)\n",
    "    valid_iterator =  su.seq2seq.make_bucket_iterator(x_valid_indexed,y_valid,batch_size,device)\n",
    "    #if fold == 0:\n",
    "    test_iterator =  su.seq2seq.make_bucket_iterator(x_test_indexed,y_test,batch_size,device)\n",
    "        \n",
    "    # Build model\n",
    "    input_size_encoder = len(word_index)\n",
    "    output_size = len(set(y_train))\n",
    "\n",
    "    encoder_net = Encoder(\n",
    "        input_size_encoder, \n",
    "        en_embedding_size, \n",
    "        hidden_size, \n",
    "        num_layers, \n",
    "        en_dropout).to(device)\n",
    "\n",
    "\n",
    "    fc_layer = FC_layer(hidden_size, output_size,fc_dropout).to(device)\n",
    "    \n",
    "    model = seq2seq(encoder_net, fc_layer).to(device)\n",
    "    \n",
    "    if load_model: # Rework has to be done for this\n",
    "        model.load_state_dict(torch.load(pretrained_model_path), strict=False)\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-4)\n",
    "    if enable_class_weight:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    # Training the network\n",
    "    # List to store values\n",
    "    train_loss_list = []\n",
    "    train_accu_list = []\n",
    "    val_loss_list = []\n",
    "    val_accu_list = []\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    train_f1_list = []\n",
    "    valid_f1_list = []\n",
    "    \n",
    "    # model training\n",
    "    loop = tqdm.tqdm(range(epochs), total=epochs,leave=False)\n",
    "    for epoch in loop:\n",
    "\n",
    "        train_loss, train_accu = su.seq2seq.train(model,criterion,optimizer,train_iterator,device)\n",
    "        val_loss,val_accu = su.seq2seq.validate(model,criterion,valid_iterator,device)\n",
    "        \n",
    "        \n",
    "        # For callback\n",
    "        # Callback saves the best model based on the below priority\n",
    "        # validation loss --> validation accuracy--> training loss--> training accuracy\n",
    "        if epoch == 0:\n",
    "            torch.save(model.state_dict(), model_output_name)\n",
    "            saved_model_id = epoch + 1\n",
    "\n",
    "        if epoch != 0:\n",
    "            current_epoch_values = [train_loss, train_accu,val_loss,val_accu]\n",
    "            previous_epoch_values = [train_loss_list,train_accu_list,val_loss_list,val_accu_list]\n",
    "            if su.callback(current_epoch_values,previous_epoch_values,model,model_output_name):\n",
    "                model_copy = copy.deepcopy(model)\n",
    "                saved_model_id = epoch + 1\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_accu_list.append(train_accu)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_accu_list.append(val_accu)\n",
    "        \n",
    "        if not trial:\n",
    "            log_file.write(str(epoch+1) + \"\\t\" + str(train_loss) + \"\\t\" + str(val_loss) + \"\\t\" + str(train_accu) + \"\\t\" + str(val_accu)  + \"\\n\")\n",
    "        loop.set_description(\"LOSS train:\" + str(train_loss) + \" val:\" + str(val_loss) + \" \\tACCU train:\" + str(train_accu) + \" val:\" + str(val_accu))\n",
    "        \n",
    "    torch.save(model_copy.state_dict(), model_output_name)\n",
    "    \n",
    "    plot_loss_accuracy(train_accu_list,val_accu_list,str(run_folder) + \"/accuracy_\" + str(fold) + \".png\")\n",
    "    plot_loss_accuracy(train_loss_list,val_loss_list,str(run_folder) + \"/loss_\" + str(fold) + \".png\")\n",
    "    \n",
    "    log_file.write(\"\\nChosen model = epoch number \" + str(saved_model_id))\n",
    "    \n",
    "    \n",
    "    # Re-initializing the model for getting statistics for all the three sets of data for the current fold\n",
    "    model = seq2seq(encoder_net, fc_layer).to(device)\n",
    "    model.load_state_dict(torch.load(model_output_name), strict=True)\n",
    "    model.to(device)\n",
    "    \n",
    "    if not trial:# classification report and confusion matrix plot\n",
    "        loss,accuracy,prediction_list = su.seq2seq.test(model,criterion,train_iterator,device)\n",
    "        image_name = str(run_folder) + \"/train_\" + str(fold) + \"_cm.png\"\n",
    "        report = su.confustion_matrix(prediction_list,image_name)\n",
    "        log_file.write(\"\\n\\n\\nTrain data : Accu-\" + str(accuracy) + \"\\tLoss-\" + str(loss) + \"\\n\")\n",
    "        log_file.write(\"Train data report \\n-\" + str(report) + \"\\n\\n\\n\\n\\n\")\n",
    "    \n",
    "        loss,accuracy,prediction_list = su.seq2seq.test(model,criterion,valid_iterator,device)\n",
    "        image_name = str(run_folder) + \"/valid_\" + str(fold) + \"_cm.png\"\n",
    "        report = su.confustion_matrix(prediction_list,image_name)\n",
    "        log_file.write(\"\\n\\n\\nValid data : Accu-\" + str(accuracy) + \"\\tLoss-\" + str(loss) + \"\\n\")\n",
    "        log_file.write(\"Valid data report \\n-\" + str(report) + \"\\n\\n\\n\\n\\n\")\n",
    "        \n",
    "        loss,accuracy,prediction_list = su.seq2seq.test(model,criterion,test_iterator,device)\n",
    "        image_name = str(run_folder) + \"/test_\" + str(fold) + \"_cm.png\"\n",
    "        report = su.confustion_matrix(prediction_list,image_name)\n",
    "        log_file.write(\"\\n\\n\\nTest data : Accu-\" + str(accuracy) + \"\\tLoss-\" + str(loss) + \"\\n\")\n",
    "        log_file.write(\"Test data report \\n-\" + str(report) + \"\\n\\n\\n\\n\\n\")\n",
    "        \n",
    "    log_file.close()\n",
    "    \n",
    "    if fold == 0 and not trial:\n",
    "        network_parameter_output.write(\"model = \" + str(model) + \"\\n\")\n",
    "        network_parameter_output.close()\n",
    "    \n",
    "    # best validation loss\n",
    "    index1 = val_loss_list.index(sorted(val_loss_list)[0]) # index of least loss\n",
    "    index2 = val_accu_list.index(sorted(val_accu_list)[-1]) # index of highest accuracy\n",
    "    print (\"Best model on loss and accuracy\")\n",
    "    print (\"LOSS train:\",train_loss_list[index1],\" val:\",val_loss_list[index1], \"\\tACCU train:\",train_accu_list[index1],\" val:\",val_accu_list[index1])\n",
    "    print (\"LOSS train:\",train_loss_list[index2],\" val:\",val_loss_list[index2], \"\\tACCU train:\",train_accu_list[index2],\" val:\",val_accu_list[index2])\n",
    "    print (\"Final model\")\n",
    "    print (\"LOSS train:\",train_loss,\" val:\",val_loss, \"\\tACCU train:\",train_accu,\" val:\",val_accu)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
