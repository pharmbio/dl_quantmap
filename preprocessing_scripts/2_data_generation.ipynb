{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import sqlite3\n",
    "import tqdm\n",
    "import  tarfile\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "sys.path.append('../supp_scripts') # Supp scripts path\n",
    "import qmap_ppi_out as qmap\n",
    "import supp_utils as su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get seed cids, ppi (protein protein interaction) cids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database information\n",
    "db_path = \"../data/qm_db.sqlite\" # Path for the database\n",
    "stitch_table_name = \"stitch_protchem_man\" # Name of the stitch table in the db\n",
    "string_table_name = \"string_protlink_man\" # Name of the string table in the db\n",
    "\n",
    "# Qunatmap parameters\n",
    "# Parameters used in quantmap paper\n",
    "chem_score = 700\n",
    "chem_max    = 10\n",
    "prot_max    = 150\n",
    "prot_score  = 700\n",
    "ppi_max     = 200\n",
    "\n",
    "# Number of workers. Taking maximum number of threads in the system.\n",
    "number_of_workers = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to database\n",
    "conn = sqlite3.connect(db_path)\n",
    "c = conn.cursor()\n",
    "\n",
    "# Get all cids from stitch\n",
    "c.execute(\"select cid from \" + str(stitch_table_name) + \";\")\n",
    "data = c.fetchall()\n",
    "data = list(set(data))\n",
    "all_cids = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get seed CIDs for quantmap, based on quality cutoff chem_score and chem_max\n",
    "seed_cid = []\n",
    "loop = tqdm.tqdm(all_cids, total=len(all_cids),leave=False)\n",
    "for cid in loop:\n",
    "    c.execute(\"select distinct protein from \" + stitch_table_name + \" where cid = \" + str(cid[0]) + \" and sc_all >=  \" + str(chem_score) + \" order by sc_all desc, \\\n",
    "              sc_exp desc limit \" + str(chem_max) + \" \")\n",
    "    if len(c.fetchall()) > 0:\n",
    "        seed_cid.append(cid[0])\n",
    "        \n",
    "# Writing obtained cids to output file\n",
    "seed_cid = sorted(seed_cid)\n",
    "outfile = open(\"../data/seed_cids.txt\",\"w\")\n",
    "for entry in seed_cid:\n",
    "    outfile.write(str(entry) + \"\\n\")\n",
    "outfile.close()\n",
    "\n",
    "print (\"Number of CIDs obtained using chem_score and chem_max cutoff = \" +  str(len(seed_cid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get seed CIDs with defined PPI cutoff for quantmap\n",
    "# Further filtering of seed CIDs using prot_score and prot_max\n",
    "seed_cid_file = open(\"../data/seed_cids.txt\",\"r\").readlines()\n",
    "\n",
    "seed_cid_file = list(map(int, seed_cid_file))\n",
    "\n",
    "ppi_cid = []\n",
    "\n",
    "loop = tqdm.tqdm(enumerate(seed_cid_file),total=len(seed_cid_file),leave=False)\n",
    "\n",
    "for i,cid in loop:\n",
    "    seeds = []\n",
    "    # The same computation is ran on above cell to obtain seed CIDs\n",
    "    # The above seed CIDs are obtained to reduce computation while obtaining ppi network\n",
    "    c.execute(\"select distinct protein from \" + stitch_table_name + \" where cid = \" + str(cid) + \" and sc_all >=  \" + str(chem_score) + \" order by sc_all desc, \\\n",
    "              sc_exp desc limit \" + str(chem_max))\n",
    "    for row in c.fetchall():\n",
    "        seeds.append(row[0])\n",
    "        \n",
    "    ppi_query = \"select pro1,pro2,sc_all/1000.0 weight from \" + string_table_name + \" where pro1 in (\" \\\n",
    "        + str(seeds)[1:-1] + \") and (pro1 < pro2 or pro2 not in (\" \\\n",
    "        + str(seeds)[1:-1] + \")) and sc_all >= \" + str(prot_score) + \" order by sc_all desc, sc_exp desc, pro1, pro2  limit \" + str(prot_max)\n",
    "\n",
    "    c.execute(ppi_query)\n",
    "    \n",
    "    if len(c.fetchall()) > 0:\n",
    "        ppi_cid.append(int(cid))\n",
    "        \n",
    "# Writing obtained cids to output file\n",
    "ppi_cid = sorted(ppi_cid)\n",
    "outfile = open(\"../data/seed_cids_with_ppi.txt\",\"w\")\n",
    "for cid in ppi_cid:\n",
    "    outfile.write(str(cid) + \"\\n\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique CIDs in STITCH = \" + str(len(all_cids)))\n",
    "print(\"Number of CIDs with seed protein = \" + str(len(seed_cid_file)))\n",
    "print(\"Number of CIDs with seed protein and have protein-protein network = \" + str(len(ppi_cid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get smiles for the cid using api\n",
    "<br>\n",
    "The below code fetches CIDs from pubchem.<br><br>The api fails/takes longer for larger datasize, hence downloading the smiles from databases are required. <br><br>\n",
    "The smiles are sanitized and converted to their canonical form using rdkit and duplicates are deleted. (This is the preprocessing of smiles).<br><br>\n",
    "The smiles for 130259 compounds are preprocessed to obtain 130127 smiles and are in \"processed_data/cid_smiles_sanitized_canonical.txt\".<br><br>\n",
    "Copy the \"processed_data/cid_smiles_sanitized_canonical.txt\" to data folder and skip the below steps till Batched quantmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_smiles = su.get_smiles_from_cid(ppi_cid,type_smiles=\"isomeric\",get_from=\"SDF\",remove_sdf=True)\n",
    "os.system(\"rm -r structure_files \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a list of smiles to its canonical form and deletes the duplicates\n",
    "def smiles_preprocessing(cid_smiles,keep_stereo=False,sanitize=True):\n",
    "    \n",
    "    cid_list = [cid for cid in cid_smiles]\n",
    "    smiles_list = [cid_smiles[cid] for cid in cid_smiles]\n",
    "    \n",
    "    canonical_cid_smiles = {}\n",
    "    for cid,smiles in zip(cid_list,smiles_list):\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles,sanitize=sanitize)\n",
    "            smiles_cleaned = Chem.MolToSmiles(mol,isomericSmiles=keep_stereo)\n",
    "            canonical_cid_smiles[cid] = smiles_cleaned\n",
    "        except:\n",
    "            smiles_cleaned = \"Null\"\n",
    "            \n",
    "    \n",
    "    unique_smiles = []\n",
    "    duplicates_count = Counter(list(canonical_cid_smiles.values()))\n",
    "    for smiles in duplicates_count:\n",
    "        if duplicates_count[smiles] == 1:\n",
    "            unique_smiles.append(smiles)\n",
    "    \n",
    "    output_cid_smiles = {}\n",
    "    for cid in canonical_cid_smiles:\n",
    "        if canonical_cid_smiles[cid] in unique_smiles:\n",
    "            output_cid_smiles[cid] = canonical_cid_smiles[cid]\n",
    "        \n",
    "    return output_cid_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_smiles_out = smiles_preprocessing(cid_smiles)\n",
    "# Saving obtained CID,smiles data\n",
    "with open(\"../data/cid_smiles_sanitized_canonical.txt\",\"w\") as f:\n",
    "    for cid in cid_smiles_out:\n",
    "        f.write(str(cid) + \" \" + cid_smiles_out[cid] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched quantmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating necessary folders and reading seed cids with ppi\n",
    "seed_cid_file = open(\"../data/seed_cids_with_ppi.txt\",\"r\").readlines()\n",
    "seed_cid_file = sorted(list(map(int, seed_cid_file)))\n",
    "os.system(\"mkdir ../data/cid_list_splits\") # To split the data to process using multiple processors\n",
    "os.system(\"mkdir ../data/ppi_results\") # To save ppi results for the seed cids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk size to process at a time.\n",
    "# Chunk size detemines number of smiles to process at a time.\n",
    "# Total smiles processed would be  \"chunk_size * number_of_workers\"\n",
    "chunk_size = 1000 # Lower this number if there is memory overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data for multiprocessing  runs\n",
    "split_number = 0\n",
    "out_cids = \"\"\n",
    "for i,entry in enumerate(seed_cid_file):\n",
    "    out_cids += (str(entry) + \"\\tdummy_text\\n\")\n",
    "    if (i + 1) % chunk_size == 0 or (i + 1) == len(seed_cid_file):\n",
    "        outfile = open(\"../data/cid_list_splits/split_\" + str(split_number) + \".txt\",\"w\")\n",
    "        outfile.write(out_cids)\n",
    "        outfile.close()\n",
    "        out_cids = \"\"\n",
    "        split_number += 1\n",
    "        \n",
    "all_cid_files = glob.glob(\"../data/cid_list_splits/*\")\n",
    "\n",
    "cids_list = []\n",
    "for filename in all_cid_files:\n",
    "    file_open = open(filename,\"r\").readlines()\n",
    "    for entry in file_open:\n",
    "        cid = entry.split()[0]\n",
    "        cids_list.append(cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quantmap as multiple process\n",
    "start_time = time.time()\n",
    "\n",
    "def run_r_script(filename):\n",
    "    file_count = str(filename.split(\".\")[-2].split(\"_\")[-1])\n",
    "    code = 'python qmap_ppi_out.py ' + str(filename) + ' ' + str(file_count) + \" \" + db_path + \" \" + stitch_table_name + \" \" + string_table_name\n",
    "    os.system(code)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    with Pool(number_of_workers) as p:\n",
    "        p.map(run_r_script, all_cid_files)\n",
    "        \n",
    "print (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spearman's footrule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load generated data to dict\n",
    "def csv_to_dict(input_file):\n",
    "    df = pd.read_csv(input_file).fillna(0)\n",
    "    df_dict = df.to_dict(orient=\"list\")\n",
    "    output_dict = {cid : {} for cid in list(df_dict.keys())[1:]}\n",
    "    enzyme_key = list(df_dict.keys())[0]\n",
    "    for cid in output_dict:\n",
    "        for i,values in enumerate(df_dict[cid]):\n",
    "            if float(values) > 0:\n",
    "                output_dict[cid][df_dict[enzyme_key][i]] = values\n",
    "    return output_dict\n",
    "\n",
    "start_time = time.time()\n",
    "ppi_files = [\"../data/ppi_results/\" + str(i) + \".csv\" for i in range(len(glob.glob(\"../data/ppi_results/*\")))]\n",
    "data_dict = {}\n",
    "cids = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(number_of_workers) as p:\n",
    "        output_dicts = p.map(csv_to_dict, ppi_files)\n",
    "\n",
    "for dicts in output_dicts:\n",
    "    data_dict.update(dicts)\n",
    "    \n",
    "sorted_dict_keys = sorted(list(map(int,list(data_dict.keys()))))\n",
    "sorted_data_dict = {}\n",
    "for key in sorted_dict_keys:\n",
    "    sorted_data_dict[key] = data_dict[str(key)]\n",
    "\n",
    "data_dict = sorted_data_dict\n",
    "    \n",
    "print (\"Loaded data\")\n",
    "print (\"Time elapsed = \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data to chunks with the given chunk size\n",
    "# Preparing for multiprocessing of spearman footrule runs on the data\n",
    "chunked_cid_list = []\n",
    "current_list = []\n",
    "calculated_cid = sorted(list(data_dict.keys()))\n",
    "for i,cid in enumerate(calculated_cid):\n",
    "    current_list.append(cid)\n",
    "    if (i + 1) % chunk_size == 0 or i + 1 == len(calculated_cid):\n",
    "        chunked_cid_list.append(current_list)\n",
    "        current_list = []\n",
    "        \n",
    "all_cid_list = calculated_cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receives two dict of compounds with their each first row as proteinid and then the ranking\n",
    "def spearman_footrule(cmpd1,cmpd2):\n",
    "    abs_diff = 0\n",
    "    no_match_count_cmpd1 = 1\n",
    "    match_count = 1\n",
    "    for lines in cmpd1:\n",
    "        eid1,rank1 = lines,cmpd1[lines]\n",
    "        if eid1 in cmpd2:\n",
    "            rank2 = cmpd2[eid1]\n",
    "            abs_diff += abs(rank1 - rank2)\n",
    "            match_count += 1\n",
    "        else:\n",
    "            no_match_count_cmpd1 += 1\n",
    "    no_match_count_cmpd2 = (len(cmpd2)-match_count)\n",
    "    no_match_count = no_match_count_cmpd1 + no_match_count_cmpd2\n",
    "    return (abs_diff + (no_match_count*(match_count+no_match_count)))\n",
    "\n",
    "\n",
    "# get spearman value for a batch of CIDs\n",
    "def process_spearman_footrule_data(input_cid_list):\n",
    "    \n",
    "    file_number = all_cid_list.index(input_cid_list[0])//chunk_size\n",
    "\n",
    "    \n",
    "    output_dict = {cid:[] for cid in input_cid_list}\n",
    "    \n",
    "    maximum = 0\n",
    "    minimum = 100000\n",
    "    for i,cid1 in enumerate(input_cid_list):\n",
    "        cmpd1 = data_dict[cid1]\n",
    "        for j,cid2 in enumerate(all_cid_list):\n",
    "            if cid1 == cid2:\n",
    "                output_dict[cid1].append(0)\n",
    "            else:\n",
    "                cmpd2 = data_dict[cid2]\n",
    "                spearman_number = spearman_footrule(cmpd1,cmpd2)\n",
    "                output_dict[cid1].append(spearman_number)\n",
    "                if spearman_number < minimum:\n",
    "                    minimum = spearman_number\n",
    "                if spearman_number > maximum:\n",
    "                    maximum = spearman_number\n",
    "    \n",
    "    output_file = open(\"../data/spearman_value_db/db_file_\" + str(file_number) + \".txt\",\"w\")\n",
    "    \n",
    "    for entry in output_dict:\n",
    "        if len(output_dict[entry]) > 0:\n",
    "            output_file.write('{\"' + str(entry) + '\":' + str(output_dict[entry]) + '}\\n')\n",
    "\n",
    "    output_file.close()\n",
    "    \n",
    "    return ([minimum,maximum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman footrule run\n",
    "os.system(\"mkdir ../data/spearman_value_db\")\n",
    "\n",
    "input_list = chunked_cid_list\n",
    "if __name__ == '__main__':\n",
    "    with Pool(number_of_workers) as p:\n",
    "        min_max_list = p.map(process_spearman_footrule_data, input_list)\n",
    "p.close()      \n",
    "\n",
    "# Get minimum and maximum spearman value for distance matrix creation\n",
    "minimum_list = []\n",
    "maximum_list = []\n",
    "for entry in min_max_list:\n",
    "    minimum_list.append(entry[0])\n",
    "    maximum_list.append(entry[1])\n",
    "minimum = min(minimum_list)\n",
    "maximum = max(maximum_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    json_dict_list = []\n",
    "    with open(filename,\"r\") as jf:\n",
    "        for json_object in jf:\n",
    "            json_dict = json.loads(json_object)\n",
    "            json_dict_list.append(json_dict)\n",
    "    output_dict_list = [{int(k):[float(i) for i in v] for k,v in dicts.items()} for dicts in json_dict_list]\n",
    "    return output_dict_list\n",
    "\n",
    "def calculate_distance_matrix(filename):\n",
    "    out_file = open(\"../data/distance_matrix/\" + filename.split(\"/\")[-1],\"w\")\n",
    "    dict_list = read_json(filename)\n",
    "    for each_dict in dict_list:\n",
    "        key = int(list(each_dict.keys())[0]) \n",
    "        value = list((np.array(list(each_dict.values())[0]).astype('float32')  - minimum) / maximum)\n",
    "        out_file.write('{\"' + str(key) + '\":' + str(value) + '}\\n')\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating distance matrix\n",
    "os.system(\"mkdir ../data/distance_matrix\")\n",
    "spearman_files = glob.glob(\"../data/spearman_value_db/*\")\n",
    "with Pool(number_of_workers) as p:\n",
    "    output = p.map(calculate_distance_matrix, spearman_files)\n",
    "p.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get order of the distance matrix file\n",
    "# Important to retain the structure of distance matrix while loading the data\n",
    "dm_files = glob.glob(\"../data/distance_matrix/*\")\n",
    "ordered_file_list = []\n",
    "for i in range(len(dm_files)):\n",
    "    ordered_file_list.append(\"../data/distance_matrix/db_file_\" + str(i) + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cids_from_file(filename):\n",
    "    dict_list = read_json(filename)\n",
    "    cid_list = []\n",
    "    for each_dict in dict_list:\n",
    "        key = int(list(each_dict.keys())[0]) \n",
    "        cid_list.append(key)\n",
    "    return (cid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CID order in distance matrix (for reference)\n",
    "cid_list = []\n",
    "for files in ordered_file_list:\n",
    "    cid_list.extend(get_cids_from_file(files))\n",
    "\n",
    "cid_order_out = open(\"../data/cid_order_file.txt\",\"w\")\n",
    "for entry in cid_list:\n",
    "    cid_order_out.write(str(entry) + \"\\n\")\n",
    "cid_order_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress data (delete the distance matrix folder, if compression is successful or running out of storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_files(filename):\n",
    "    os.system(\"mkdir ../data/\" + str(filename.split(\"/\")[-2]) + \"_compressed\")\n",
    "    os.system(\"tar -czvf ../data/\" + str(filename.split(\"/\")[-2]) + \"_compressed/\" + str(filename.split(\"/\")[-1]) + \".tar \" + filename)\n",
    "    \n",
    "dm_files = glob.glob(\"../data/distance_matrix/*\")\n",
    "with Pool(number_of_workers) as p:\n",
    "    p.map(compress_files, dm_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading compressed files (Memory intensive process)\n",
    "### Recommended to skip the below process and clustering\n",
    "### Instead use the provided data\n",
    "### Below process is done in a system having ~1TB of memory\n",
    "<br><br>\n",
    "To use provided data, copy \"cluster_results\", \"cid_order_file.txt\" and \"cid_smiles_sanitized_canonical.txt\" from \"processed_data\" to the \"data\" folder. <br>\n",
    "And skip all the below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_tar(filename):\n",
    "    tar = tarfile.open(filename)\n",
    "    for member in tar.getmembers():\n",
    "        f=tar.extractfile(member)\n",
    "    content= f.read()\n",
    "    str_content = content.decode(\"utf-8\")\n",
    "    str_content_split = str_content.split(\"\\n\")\n",
    "    return (str_content_split)\n",
    "\n",
    "def read_json_from_tar(filename):\n",
    "    json_dict_list = []\n",
    "    for json_object in read_from_tar(filename):\n",
    "        if len(json_object) > 0:\n",
    "            json_dict = json.loads(json_object)\n",
    "            json_dict_list.append(json_dict)\n",
    "    \n",
    "    output_dict_list = [{int(k):[float(i) for i in v] for k,v in dicts.items()} for dicts in json_dict_list]\n",
    "    return output_dict_list\n",
    "\n",
    "def get_distance_matrix(filename):\n",
    "    output_list = []\n",
    "    for each_dict in read_json_from_tar(filename):\n",
    "        output_list.append(list(each_dict.values())[0])\n",
    "    file_number = filename.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]\n",
    "    return (file_number,np.array(output_list))\n",
    "\n",
    "print (\"Loaded functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading distance matrix\n",
    "ordered_file_list = [\"../data/distance_matrix_compressed/db_file_\" + str(i) + \".txt.tar\" for i in range(len(glob.glob(\"../data/distance_matrix_compressed/*\")))]\n",
    "cid_order = list(map(lambda x:int(x),open(\"../data/cid_order_file.txt\",\"r\").readlines()))\n",
    "\n",
    "print (\"Filenames created\")\n",
    "\n",
    "print (\"Loading distance matrix\")\n",
    "if __name__ == '__main__':\n",
    "    with Pool(number_of_workers) as p:\n",
    "        output = p.map(get_distance_matrix, ordered_file_list)\n",
    "        \n",
    "output_file_order = [int(entry[0]) for entry in output]\n",
    "for i in range(len(output_file_order)):\n",
    "    if i == 0:\n",
    "        dm = output[output_file_order.index(i)][1]\n",
    "    else:\n",
    "        dm = np.concatenate((dm,output[output_file_order.index(i)][1]),axis=0)\n",
    "        \n",
    "print (\"Loaded distance matrix\")\n",
    "del output\n",
    "print (\"Deleted distance matrix copy from memory (SAVED MEMORY)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-mean clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_clustering(distance_threshold):\n",
    "    print (\"\\n\\n\\nClustering started for \" + str(distance_threshold) )\n",
    "    cluster = AgglomerativeClustering(n_clusters=None, affinity='precomputed', linkage='average',distance_threshold=distance_threshold)\n",
    "    clusters_list = cluster.fit_predict(dm)\n",
    "    \n",
    "    with open(\"clustering_details_\" + str(distance_threshold) + \".csv\",\"w\") as of:\n",
    "        ii = itertools.count(dm.shape[0])\n",
    "        cluster_distances = cluster.distances_\n",
    "        node_details = [{'node_id': next(ii), 'left': x[0], 'right':x[1], 'distance' : cluster_distances[i]} for i,x in enumerate(cluster.children_)]\n",
    "        of.write(\"node_id,left,right,distance\\n\")\n",
    "        for dicts in node_details:\n",
    "            node_id = dicts[\"node_id\"]\n",
    "            left = dicts[\"left\"]\n",
    "            right = dicts[\"right\"]\n",
    "            distance = dicts[\"distance\"]\n",
    "            of.write(str(node_id) + \",\" + str(left) + \",\" + str(right) + \",\" + str(distance) + \"\\n\")\n",
    "    \n",
    "    print (\"Clustering finished for \" + str(distance_threshold) )\n",
    "    with open(\"cid_cluster_\" + str(distance_threshold) + \".txt\",\"w\") as of:\n",
    "        for i,cid in enumerate(cid_order):\n",
    "            of.write(str(cid) + \" \" + str(clusters_list[i]) + \"\\n\")\n",
    "    print (\"Saved CID-cluster data for \" + str(distance_threshold) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The clustering is done for distance thresholds of 0.001, 0.005, 0.01, 0.05 and 0.1\n",
    "os.system(\"mkdir ../data/cluster_results\")\n",
    "do_clustering(0.001)\n",
    "do_clustering(0.005)\n",
    "do_clustering(0.01)\n",
    "do_clustering(0.05)\n",
    "do_clustering(0.1)\n",
    "\n",
    "os.system(\"mv clustering_details* ../data/cluster_results\")\n",
    "os.system(\"mv cid_cluster_* ../data/cluster_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qmpred",
   "language": "python",
   "name": "qmpred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
