{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get seed cids, ppi cids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database information\n",
    "db_path = \"\"\n",
    "stitch_table_name = \"\"\n",
    "string_table_name = \"\"\n",
    "\n",
    "# Qunatmap parameters\n",
    "chem_score = 700\n",
    "chem_max    = 10\n",
    "prot_max    = 150\n",
    "prot_score  = 700\n",
    "ppi_max     = 200\n",
    "number_of_workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('stitch_protchem_man',)\n",
      "('string_protlink_man',)\n",
      "('akshai_chem_hubs',)\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(db_path)\n",
    "c = conn.cursor()\n",
    "\n",
    "# Get all cids from stitch\n",
    "c.execute(\"select cid from \" + str(stitch_table_name) + \";\")\n",
    "data = c.fetchall()\n",
    "data = list(set(data))\n",
    "all_cids = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Get seed CIDs for quantmap\n",
    "seed_cid = []\n",
    "loop = tqdm.tqdm(all_cids, total=len(all_cids),leave=False)\n",
    "for cid in loop:\n",
    "    c.execute(\"select distinct protein from \" + stitch_table_name + \" where cid = \" + str(cid[0]) + \" and sc_all >=  \" + str(chem_score) + \" order by sc_all desc, \\\n",
    "              sc_exp desc limit \" + str(chem_max) + \" \")\n",
    "    if len(c.fetchall()) > 0:\n",
    "        seed_cid.append(cid[0])\n",
    "        \n",
    "seed_cid = sorted(seed_cid)\n",
    "outfile = open(\"seed_cids.txt\",\"w\")\n",
    "for entry in seed_cid:\n",
    "    outfile.write(str(entry) + \"\\n\")\n",
    "outfile.close()\n",
    "\n",
    "print (len(seed_cid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    }
   ],
   "source": [
    "# Get seed CIDs with defined PPI cutoff for quantmap\n",
    "seed_cid_file = open(\"seed_cids.txt\",\"r\").readlines()\n",
    "\n",
    "seed_cid_file = list(map(int, seed_cid_file))\n",
    "\n",
    "ppi_cid = []\n",
    "\n",
    "loop = tqdm.tqdm(enumerate(seed_cid_file),total=len(seed_cid_file),leave=False)\n",
    "\n",
    "for i,cid in loop:\n",
    "    seeds = []\n",
    "    c.execute(\"select distinct protein from \" + stitch_table_name + \" where cid = \" + str(cid) + \" and sc_all >=  \" + str(chem_score) + \" order by sc_all desc, \\\n",
    "              sc_exp desc limit \" + str(chem_max))\n",
    "    for row in c.fetchall():\n",
    "        seeds.append(row[0])\n",
    "        \n",
    "    ppi_query = \"select pro1,pro2,sc_all/1000.0 weight from \" + string_table_name + \" where pro1 in (\" \\\n",
    "        + str(seeds)[1:-1] + \") and (pro1 < pro2 or pro2 not in (\" \\\n",
    "        + str(seeds)[1:-1] + \")) and sc_all >= \" + str(prot_score) + \" order by sc_all desc, sc_exp desc, pro1, pro2  limit \" + str(prot_max)\n",
    "\n",
    "    c.execute(ppi_query)\n",
    "    \n",
    "    if len(c.fetchall()) > 0:\n",
    "        ppi_cid.append(int(cid))\n",
    "        \n",
    "ppi_cid = sorted(ppi_cid)\n",
    "outfile = open(\"seed_cids_with_ppi.txt\",\"w\")\n",
    "for cid in ppi_cid:\n",
    "    outfile.write(str(cid) + \"\\n\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique CIDs = \" + str(len(all_cids)))\n",
    "print(\"Number of CIDs with seed protein = \" + str(len(seed_cid_file)))\n",
    "print(\"Number of CIDs with seed protein and have protein-protein network= \" + str(len(ppi_cid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get smiles for the cid using api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('//') # Supp scripts path\n",
    "import supp_utils as su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_smiles = su.get_smiles_from_cid(ppi_cid,type_smiles=\"isomeric\",get_from=\"SDF\",remove_sdf=True)\n",
    "# if all smiles are not found use below\n",
    "# rest_cids = []\n",
    "# for cid in ppi_cid:\n",
    "#    try:\n",
    "#        cid_smiles[cid]\n",
    "#    except:\n",
    "#        rest_cids.append(cid)\n",
    "# rest_cid_smiles = su.get_smiles_from_cid(rest_cids,type_smiles=\"isomeric\",get_from=\"canonical\",save_output=False,remove_sdf=True)\n",
    "# cid_smiles.update(rest_cid_smiles("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cid_smiles.txt\",\"w\") as f:\n",
    "    for cid in cid_smiles:\n",
    "        f.write(str(cid) + \" \" + cid_smiles_human[cid] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched quantmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import tqdm\n",
    "import sqlite3\n",
    "import json\n",
    "#import yaml\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import qmap_ppi_out as qmap\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['which', 'python'], returncode=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_cur = os.environ.copy()\n",
    "path_cur = '' # environment path for quantmap\n",
    "subprocess.run(['which', 'python'], env={'PATH': path_cur})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_cid_file = open(\"seed_cids_with_ppi.txt\",\"r\").readlines()\n",
    "seed_cid_file = sorted(list(map(int, seed_cid_file)))\n",
    "os.system(\"mkdir cid_list_splits\")\n",
    "os.system(\"mkdir ppi_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiprocessing splits of qunatmap runs\n",
    "split_number = 0\n",
    "batch_size = 1000\n",
    "out_cids = \"\"\n",
    "for i,entry in enumerate(seed_cid_file):\n",
    "    out_cids += (str(entry) + \"\\tdummy_text\\n\")\n",
    "    if (i + 1) % batch_size == 0 or (i + 1) == len(seed_cid_file):\n",
    "        outfile = open(\"cid_list_splits/split_\" + str(split_number) + \".txt\",\"w\")\n",
    "        outfile.write(out_cids)\n",
    "        outfile.close()\n",
    "        out_cids = \"\"\n",
    "        split_number += 1\n",
    "        \n",
    "all_cid_files = glob.glob(\"cid_list_splits/*\")\n",
    "\n",
    "cids_list = []\n",
    "for filename in all_cid_files:\n",
    "    file_open = open(filename,\"r\").readlines()\n",
    "    for entry in file_open:\n",
    "        cid = entry.split()[0]\n",
    "        cids_list.append(cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225.81683087348938\n"
     ]
    }
   ],
   "source": [
    "# Run quantmap\n",
    "start_time = time.time()\n",
    "\n",
    "def run_r_script(filename):\n",
    "    file_count = str(filename.split(\".\")[-2].split(\"_\")[-1])\n",
    "    code = 'source ~/.bashrc && conda activate qmap_data && python qmap_ppi_out.py ' + str(filename) + ' ' + str(file_count)\n",
    "    subprocess.run(\"bash -c '\" + code + \"'\", shell=True)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    with Pool(number_of_workers) as p:\n",
    "        p.map(run_r_script, all_cid_files)\n",
    "        \n",
    "print (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spearman's footrule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import tqdm\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load generated data to dict\n",
    "def csv_to_dict(input_file):\n",
    "    df = pd.read_csv(input_file).fillna(0)\n",
    "    df_dict = df.to_dict(orient=\"list\")\n",
    "    output_dict = {cid : {} for cid in list(df_dict.keys())[1:]}\n",
    "    enzyme_key = list(df_dict.keys())[0]\n",
    "    for cid in output_dict:\n",
    "        for i,values in enumerate(df_dict[cid]):\n",
    "            if float(values) > 0:\n",
    "                output_dict[cid][df_dict[enzyme_key][i]] = values\n",
    "    return output_dict\n",
    "\n",
    "start_time = time.time()\n",
    "ppi_files = [\"ppi_results/\" + str(i) + \".csv\" for i in range(len(glob.glob(\"ppi_results/*\")))]\n",
    "data_dict = {}\n",
    "cids = []\n",
    "#loop = tqdm.tqdm(ppi_files,total=len(ppi_files),leave=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(20) as p:\n",
    "        output_dicts = p.map(csv_to_dict, ppi_files)\n",
    "\n",
    "for dicts in output_dicts:\n",
    "    data_dict.update(dicts)\n",
    "    \n",
    "sorted_dict_keys = sorted(list(map(int,list(data_dict.keys()))))\n",
    "sorted_data_dict = {}\n",
    "for key in sorted_dict_keys:\n",
    "    sorted_data_dict[key] = data_dict[str(key)]\n",
    "\n",
    "data_dict = sorted_data_dict\n",
    "    \n",
    "print (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk size for  input for multiprocessing pool\n",
    "chunk_size = 1000\n",
    "chunked_cid_list = []\n",
    "current_list = []\n",
    "calculated_cid = sorted(list(data_dict.keys()))\n",
    "for i,cid in enumerate(calculated_cid):\n",
    "    current_list.append(cid)\n",
    "    if (i + 1) % chunk_size == 0 or i + 1 == len(calculated_cid):\n",
    "        chunked_cid_list.append(current_list)\n",
    "        current_list = []\n",
    "        \n",
    "all_cid_list = calculated_cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receives two dict of compounds with their each first row as enzymeid and then the ranking\n",
    "def spearman_footrule(cmpd1,cmpd2):\n",
    "    abs_diff = 0\n",
    "    no_match_count_cmpd1 = 1\n",
    "    match_count = 1\n",
    "    for lines in cmpd1:\n",
    "        eid1,rank1 = lines,cmpd1[lines]\n",
    "        if eid1 in cmpd2:\n",
    "            rank2 = cmpd2[eid1]\n",
    "            abs_diff += abs(rank1 - rank2)\n",
    "            match_count += 1\n",
    "        else:\n",
    "            no_match_count_cmpd1 += 1\n",
    "    no_match_count_cmpd2 = (len(cmpd2)-match_count)\n",
    "    no_match_count = no_match_count_cmpd1 + no_match_count_cmpd2\n",
    "    return (abs_diff + (no_match_count*(match_count+no_match_count)))\n",
    "\n",
    "\n",
    "# get spearman value for batch of cid\n",
    "def process_spearman_footrule_data(input_cid_list):\n",
    "    \n",
    "    file_number = all_cid_list.index(input_cid_list[0])//chunk_size\n",
    "\n",
    "    \n",
    "    output_dict = {cid:[] for cid in input_cid_list}\n",
    "    \n",
    "    maximum = 0\n",
    "    minimum = 100000\n",
    "    for i,cid1 in enumerate(input_cid_list):\n",
    "        cmpd1 = data_dict[cid1]\n",
    "        for j,cid2 in enumerate(all_cid_list):\n",
    "            if cid1 == cid2:\n",
    "                output_dict[cid1].append(0)\n",
    "            else:\n",
    "                cmpd2 = data_dict[cid2]\n",
    "                spearman_number = spearman_footrule(cmpd1,cmpd2)\n",
    "                output_dict[cid1].append(spearman_number)\n",
    "                if spearman_number < minimum:\n",
    "                    minimum = spearman_number\n",
    "                if spearman_number > maximum:\n",
    "                    maximum = spearman_number\n",
    "    \n",
    "    output_file = open(\"spearman_value_db/db_file_\" + str(file_number) + \".txt\",\"w\")\n",
    "    \n",
    "    for entry in output_dict:\n",
    "        if len(output_dict[entry]) > 0:\n",
    "            output_file.write('{\"' + str(entry) + '\":' + str(output_dict[entry]) + '}\\n')\n",
    "\n",
    "    output_file.close()\n",
    "    \n",
    "    return ([minimum,maximum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38026.794129133224\n"
     ]
    }
   ],
   "source": [
    "os.system(\"mkdir spearman_value_db\")\n",
    "\n",
    "start_time = time.time()\n",
    "input_list = chunked_cid_list\n",
    "if __name__ == '__main__':\n",
    "    with Pool(number_of_workers) as p:\n",
    "        min_max_list = p.map(process_spearman_footrule_data, input_list)\n",
    "p.close()      \n",
    "print (time.time() - start_time)\n",
    "\n",
    "# Get minimum and maximum spearman value for distance matrix creation\n",
    "minimum_list = []\n",
    "maximum_list = []\n",
    "for entry in min_max_list:\n",
    "    minimum_list.append(entry[0])\n",
    "    maximum_list.append(entry[1])\n",
    "minimum = min(minimum_list)\n",
    "maximum = max(maximum_list)\n",
    "print (minimum,maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import time\n",
    "import tqdm\n",
    "import os\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    print (filename)\n",
    "    json_dict_list = []\n",
    "    with open(filename,\"r\") as jf:\n",
    "        for json_object in jf:\n",
    "            json_dict = json.loads(json_object)\n",
    "            json_dict_list.append(json_dict)\n",
    "    output_dict_list = [{int(k):[float(i) for i in v] for k,v in dicts.items()} for dicts in json_dict_list]\n",
    "    return output_dict_list\n",
    "\n",
    "def calculate_distance_matrix(filename):\n",
    "    out_file = open(\"distance_matrix/\" + filename.split(\"/\")[1],\"w\")\n",
    "    dict_list = read_json(filename)\n",
    "    for each_dict in dict_list:\n",
    "        key = int(list(each_dict.keys())[0]) \n",
    "        value = list((np.array(list(each_dict.values())[0]).astype('float32')  - minimum) / maximum)\n",
    "        out_file.write('{\"' + str(key) + '\":' + str(value) + '}\\n')\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"mkdir distance_matrix\")\n",
    "\n",
    "with Pool(number_of_workers) as p:\n",
    "    output = p.map(calculate_distance_matrix, spearman_files)\n",
    "p.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_files = glob.glob(\"distance_matrix/*\")\n",
    "ordered_file_list = []\n",
    "for i in range(len(dm_files)):\n",
    "    ordered_file_list.append(\"distance_matrix/db_file_\" + str(i) + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cids_from_file(filename):\n",
    "    dict_list = read_json(filename)\n",
    "    cid_list = []\n",
    "    for each_dict in dict_list:\n",
    "        key = int(list(each_dict.keys())[0]) \n",
    "        cid_list.append(key)\n",
    "    return (cid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_list = []\n",
    "for files in ordered_file_list:\n",
    "    cid_list.extend(get_cids_from_file(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CID order in distance matrix (for reference)\n",
    "cid_order_out = open(\"cid_order_file.txt\",\"w\")\n",
    "for entry in cid_list:\n",
    "    cid_order_out.write(str(entry) + \"\\n\")\n",
    "cid_order_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress data (delete the distance matrix folder, if compression is successful or running out of storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  tarfile\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import tqdm\n",
    "import sqlite3\n",
    "import json\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_files(filename):\n",
    "    os.system(\"mkdir \" + str(filename.split(\"/\")[-2]) + \"_compressed\")\n",
    "    os.system(\"tar -czvf \" + str(filename.split(\"/\")[-2]) + \"_compressed/\" + str(filename.split(\"/\")[-1]) + \".tar \" + filename)\n",
    "    \n",
    "dm_files = glob.glob(\"distance_matrix/*\")\n",
    "with Pool(number_of_workers) as p:\n",
    "    p.map(compress_files, dm_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading compressed files (Memory intensive process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_file_list = [\"distance_matrix_compressed/db_file_\" + str(i) + \".txt.tar\" for i in range(len(glob.glob(\"distance_matrix_compressed/*\")))]\n",
    "cid_order = list(map(lambda x:int(x),open(\"cid_order_file.txt\",\"r\").readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_tar(filename):\n",
    "    print (\"\\n\\n\\nUnzipping file = \" + str(filename))\n",
    "    tar = tarfile.open(filename)\n",
    "    inside_filename = filename.split(\"/\")[-2][:-11] + \"/\" + filename.split(\"/\")[-1][:-4]\n",
    "    member = tar.getmember(inside_filename)\n",
    "    f = tar.extractfile(member)\n",
    "    content= f.read()\n",
    "    str_content = content.decode(\"utf-8\")\n",
    "    str_content_split = str_content.split(\"\\n\")\n",
    "    print (\"Unzipped file = \" + str(filename))\n",
    "    return (str_content_split)\n",
    "\n",
    "def read_json_from_tar(filename):\n",
    "    json_dict_list = []\n",
    "    for json_object in read_from_tar(filename):\n",
    "        if len(json_object) > 0:\n",
    "            json_dict = json.loads(json_object)\n",
    "            json_dict_list.append(json_dict)\n",
    "    \n",
    "    output_dict_list = [{int(k):[float(i) for i in v] for k,v in dicts.items()} for dicts in json_dict_list]\n",
    "    return output_dict_list\n",
    "\n",
    "def get_distance_matrix(filename):\n",
    "    output_list = []\n",
    "    for each_dict in read_json_from_tar(filename):\n",
    "        output_list.append(list(each_dict.values())[0])\n",
    "    file_number = filename.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]\n",
    "    return (file_number,np.array(output_list))\n",
    "\n",
    "print (\"Loaded functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_file_list = [\"distance_matrix_compressed/db_file_\" + str(i) + \".txt.tar\" for i in range(len(glob.glob(\"distance_matrix_compressed/*\")))]\n",
    "cid_order = list(map(lambda x:int(x),open(\"cid_order_file.txt\",\"r\").readlines()))\n",
    "\n",
    "print (\"Filenames created\")\n",
    "\n",
    "print (\"Loading distance matrix\")\n",
    "if __name__ == '__main__':\n",
    "    with Pool(number_of_workers) as p:\n",
    "        output = p.map(get_distance_matrix, ordered_file_list)\n",
    "        \n",
    "output_file_order = [int(entry[0]) for entry in output]\n",
    "for i in range(len(output_file_order)):\n",
    "    if i == 0:\n",
    "        dm = output[output_file_order.index(i)][1]\n",
    "    else:\n",
    "        dm = np.concatenate((dm,output[output_file_order.index(i)][1]),axis=0)\n",
    "        \n",
    "print (\"Loaded distance matrix\")\n",
    "del output\n",
    "print (\"Deleted distance matrix copy (SAVED MEMORY)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-mean clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_clustering(distance_threshold):\n",
    "    print (\"\\n\\n\\nClustering started for \" + str(distance_threshold) )\n",
    "    cluster = AgglomerativeClustering(n_clusters=None, affinity='precomputed', linkage='average',distance_threshold=distance_threshold)\n",
    "    clusters_list = cluster.fit_predict(dm)\n",
    "    \n",
    "    with open(\"clustering_details_\" + str(distance_threshold) + \".csv\",\"w\") as of:\n",
    "        ii = itertools.count(dm.shape[0])\n",
    "        cluster_distances = cluster.distances_\n",
    "        node_details = [{'node_id': next(ii), 'left': x[0], 'right':x[1], 'distance' : cluster_distances[i]} for i,x in enumerate(cluster.children_)]\n",
    "        of.write(\"node_id,left,right,distance\\n\")\n",
    "        for dicts in node_details:\n",
    "            node_id = dicts[\"node_id\"]\n",
    "            left = dicts[\"left\"]\n",
    "            right = dicts[\"right\"]\n",
    "            distance = dicts[\"distance\"]\n",
    "            of.write(str(node_id) + \",\" + str(left) + \",\" + str(right) + \",\" + str(distance) + \"\\n\")\n",
    "    \n",
    "    print (\"Clustering finished for \" + str(distance_threshold) )\n",
    "    with open(\"cid_cluster_\" + str(distance_threshold) + \".txt\",\"w\") as of:\n",
    "        for i,cid in enumerate(cid_order):\n",
    "            of.write(str(cid) + \" \" + str(clusters_list[i]) + \"\\n\")\n",
    "    print (\"Saved CID-cluster data for \" + str(distance_threshold) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_clustering(0.001)\n",
    "do_clustering(0.005)\n",
    "do_clustering(0.01)\n",
    "do_clustering(0.05)\n",
    "do_clustering(0.1)\n",
    "do_clustering(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
