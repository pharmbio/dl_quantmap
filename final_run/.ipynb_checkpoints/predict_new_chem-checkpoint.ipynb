{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import sys\n",
    "import random\n",
    "import string\n",
    "import tqdm\n",
    "import json\n",
    "import time\n",
    "import sqlite3\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit import RDLogger\n",
    "\n",
    "from SmilesPE.pretokenizer import atomwise_tokenizer\n",
    "from SmilesPE.pretokenizer import kmer_tokenizer\n",
    "from SmilesPE.spe2vec import Corpus\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "#from utils import *\n",
    "import torch\n",
    "\n",
    "sys.path.append('//')\n",
    "import supp_utils as su\n",
    "\n",
    "#torch.cuda.set_device(0) #change to 0 if you only has one GPU\n",
    "# set gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device,torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_test = \"\" # Format cid in each line or cid - name in each line\n",
    "cluster_distance_file = \"\"\n",
    "\n",
    "\n",
    "Number_of_workers = 8\n",
    "gpu_id = 0\n",
    "if gpu_id != None:\n",
    "    device = \"cuda:\" + str(gpu_id)\n",
    "else:\n",
    "    gpu_id = 0\n",
    "\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "spe_token_path = \"pretraining_tokens.txt\"\n",
    "\n",
    "tokenization = \"SPE\"\n",
    "model_path = \"models/\"\n",
    "pretraining_new_wt = \"_model_clas\"\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# To remove rdkit warning\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"protein_function_0.005_renamed.yaml\", 'r') as stream:\n",
    "    protein_function = yaml.safe_load(stream)\n",
    "    \n",
    "with open(\"protein_function_0.005_large_sc_all_500.yaml\", 'r') as stream:\n",
    "    protein_function_large = yaml.safe_load(stream)\n",
    "    \n",
    "cluster_distance = pd.read_csv(cluster_distance_file,header=None,names=[\"cluster1\",\"cluster2\",\"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cids = []\n",
    "names = []\n",
    "for entry in open(input_file_test,\"r\").readlines():\n",
    "    cid = entry.split()[0]\n",
    "    input_cids.append(cid)\n",
    "    try:\n",
    "        name = entry.split()[1]\n",
    "        if len(entry.split()) > 2:\n",
    "            name = ' '.join(entry.split()[1:])\n",
    "        names.append(name)\n",
    "    except:\n",
    "        pass\n",
    "cid_smiles = su.get_smiles_from_cid(input_cids,type_smiles=\"isomeric\",get_from=\"SDF\",folder_file_name=\"\",save_output=False,remove_sdf=False)\n",
    "test_df = pd.DataFrame([(cid_smiles[cid],cid) for cid in cid_smiles], columns=['Smiles','CID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_filename = \"cid_cluster_0.005_renamed.txt\"\n",
    "def check_presence_of_cid_in_train_valid_set(input_cid_list,train_file,valid_file=None):\n",
    "    cid_train = [int(entry.split()[0]) for entry in open(train_valid_filename,\"r\").readlines() if len(entry) > 0]\n",
    "    already_present = []\n",
    "    for cid in input_cid_list:\n",
    "        if cid in cid_train:\n",
    "            already_present.append(cid)\n",
    "    return (already_present,cid_train)\n",
    "\n",
    "already_present_train,cid_train = check_presence_of_cid_in_train_valid_set(list(map(int,(list(cid_smiles.keys())))),train_valid_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(yhat,y):\n",
    "    softmax = torch.exp(yhat.float())\n",
    "    prob = softmax.cpu().detach().numpy()\n",
    "    predictions = np.argmax(prob, axis=1)\n",
    "    y_truth = y.cpu().detach().numpy()\n",
    "    accuracy_check = (y_truth==predictions)\n",
    "    count = np.count_nonzero(accuracy_check)\n",
    "    accuracy = (count/len(accuracy_check))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenization == \"SPE\":\n",
    "    MolTokenizer = su.molpmofit.MolTokenizer_spe_sos_eos\n",
    "    tok = Tokenizer(partial(MolTokenizer,token_path=spe_token_path), n_cpus=Number_of_workers, pre_rules=[], post_rules=[])\n",
    "else:\n",
    "    MolTokenizer = su.molpmofit.MolTokenizer_atomwise_sos_eos\n",
    "    tok = Tokenizer(partial(MolTokenizer), n_cpus=Number_of_workers, pre_rules=[], post_rules=[])\n",
    "\n",
    "tok = Tokenizer(partial(MolTokenizer,token_path=spe_token_path), n_cpus=Number_of_workers, pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [vocab_token.strip() for vocab_token in open(\"models/text_class_vocab.txt\",\"r\").readlines()]\n",
    "vocab_class = text.transform.Vocab(vocab)\n",
    "test_data_clas = TextClasDataBunch.from_df(\"\", test_df, test_df, bs=batch_size, tokenizer=tok, \n",
    "                              chunksize=50000, text_cols='Smiles',label_cols='CID', vocab=vocab_class, max_vocab=60000,\n",
    "                                              include_bos=False,classes=[i for i in range(1,242)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = text_classifier_learner(test_data_clas, AWD_LSTM, pretrained=False, drop_mult=0.2)\n",
    "learner.load('_model_clas', purge=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of clusters\n",
    "cid_prediction = {}\n",
    "\n",
    "for i,cid in enumerate(cid_smiles):\n",
    "    smiles = cid_smiles[cid]\n",
    "    results = learner.predict(smiles)\n",
    "    prob = results[2].cpu().detach().numpy()\n",
    "    predictions = results[1].cpu().detach().numpy().tolist()\n",
    "    if len(names) > 0:\n",
    "        cid_prediction[cid] = {\"prediction\":predictions,\"name\":names[i],\"softmax_probability\":max(prob)}\n",
    "    else:\n",
    "        cid_prediction[cid] = {\"prediction\":predictions,\"softmax_probability\":max(prob)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distance between clusters\n",
    "predicted_clusters = []\n",
    "for cid in cid_prediction:\n",
    "    cluster = cid_prediction[cid][\"prediction\"]\n",
    "    predicted_clusters.append(cluster)\n",
    "\n",
    "cluster_distance_dicts = []\n",
    "for clust1 in predicted_clusters:\n",
    "    for clust2 in predicted_clusters:\n",
    "        if clust1 != clust2:\n",
    "            distance = cluster_distance[((cluster_distance[\"cluster1\"] == clust1) | (cluster_distance[\"cluster2\"] == clust1)) & \\\n",
    "                             ((cluster_distance[\"cluster1\"] == clust2) | (cluster_distance[\"cluster2\"] == clust2))][\"distance\"].tolist()[0]\n",
    "        else:\n",
    "            distance = 0\n",
    "        entry1 = {\"cluster1\":clust1,\"cluster2\":clust2,\"distance\":distance}\n",
    "        entry2 = {\"cluster1\":clust2,\"cluster2\":clust1,\"distance\":distance}\n",
    "        if entry1 not in cluster_distance_dicts:\n",
    "            cluster_distance_dicts.append(entry1)\n",
    "        if entry2 not in cluster_distance_dicts:\n",
    "            cluster_distance_dicts.append(entry2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distance between cids\n",
    "cid_distance_dicts = []\n",
    "for cid1 in cid_prediction:\n",
    "    for cid2 in cid_prediction:\n",
    "        clust1 = cid_prediction[cid1][\"prediction\"]\n",
    "        clust2 = cid_prediction[cid2][\"prediction\"]\n",
    "        for lists in cluster_distance_dicts:\n",
    "            try:\n",
    "                if lists['cluster1'] == clust1 and lists['cluster2'] == clust2:\n",
    "                    distance = lists[\"distance\"]\n",
    "                    cid_distance = {\"cid1\":cid1,\"cid2\":cid2,\"distance\":distance}\n",
    "                    cid_distance_dicts.append(cid_distance)\n",
    "                    break\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make distance matrix\n",
    "cid_distance_matrix = []\n",
    "for cid1 in input_cids:\n",
    "    row_list = []\n",
    "    for cid2 in input_cids:\n",
    "        for lists in cid_distance_dicts:\n",
    "            try:\n",
    "                if lists['cid1'] == int(cid1) and lists['cid2'] == int(cid2):\n",
    "                    distance = lists[\"distance\"]\n",
    "                    if distance == 0 and cid1 != cid2:\n",
    "                        distance += 0.001\n",
    "                    row_list.append(distance)\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "    cid_distance_matrix.append(row_list)\n",
    "dm_array = np.array(cid_distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "def make_plot(distance_matrix,labels,figure_name=\"chemical_distance.png\"):\n",
    "    dists = squareform(distance_matrix)\n",
    "    links = linkage(dists, \"complete\")\n",
    "    ddata = dendrogram(links, labels=labels,leaf_font_size=12,orientation=\"left\") \n",
    "    for i, d in zip(ddata['icoord'], ddata['dcoord']):\n",
    "            y = 0.5 * sum(i[1:3])\n",
    "            x = d[1]\n",
    "            if x > 0.001:\n",
    "                #plt.plot(x, y, 'ro')\n",
    "                plt.annotate(\"%.3g\" % x, (x, y), xytext=(0, +12),\n",
    "                                 textcoords='offset points',\n",
    "                                 va='top', ha='center',fontsize=12)\n",
    "\n",
    "    plt.xlabel(\"Distance\",fontsize=12)\n",
    "    plt.ylabel(\"Chemicals\",fontsize=12)\n",
    "    plt.title(\"Chemical distance\",fontsize=15)#, orientation='left'\n",
    "    plt.savefig(figure_name)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(dm_array,input_cids,\"chemical_distance_cids.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(names) > 0:\n",
    "    make_plot(dm_array,names,\"chemical_distance_names.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Club predictions\n",
    "cluster_cid = {}\n",
    "for cid in cid_prediction:\n",
    "    predicted_cluster = cid_prediction[cid][\"prediction\"]\n",
    "    if predicted_cluster not in cluster_cid:\n",
    "        cluster_cid[predicted_cluster] = []\n",
    "    cluster_cid[predicted_cluster].append(cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get function\n",
    "def get_function_from_prediction(protein_function_file):\n",
    "    for cluster in cluster_cid:\n",
    "        cids = cluster_cid[cluster]\n",
    "        print (\"CIDs = \" + str(cids)[1:-1])\n",
    "        if len(names) > 0:\n",
    "            chemical_names = []\n",
    "            for cid in cids:\n",
    "                index = input_cids.index(str(cid))\n",
    "                chemical_names.append(names[index])\n",
    "            print (\"Chemical names = \" + str(chemical_names)[1:-1])\n",
    "        print (\"Cluster predicted= \" + str(cluster))\n",
    "        print (\"\\nFunction\\n\")\n",
    "        for i,entry in enumerate(protein_function_file[cluster]):\n",
    "            print (str(i+1) + \".) \" + str(entry)[1:-1] + \"\\n\")\n",
    "        print (\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_function_from_prediction(protein_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_function_from_prediction(protein_function_large)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molpmofit",
   "language": "python",
   "name": "molpmofit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
